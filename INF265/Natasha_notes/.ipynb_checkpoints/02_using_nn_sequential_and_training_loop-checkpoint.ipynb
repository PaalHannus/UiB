{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Define, train and evaluate a basic Neural Network in Pytorch\n",
    "\n",
    "#### Introductory note \n",
    "\n",
    "These tutorials are inspired by the book \"[Deep Learning with PyTorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\" by Stevens et al. They can be seen as a summary of the part I of book regarding PyTorch itself. Normally, following the tutorials should be enough and reading the book is not required. But of course, if you are interested and curious you can try to follow the book while reading these tutorials. I tried to associate the most important part of these tutorials with their respective book sections. Some other parts of the tutorials have been done from scratch or inspired by the PyTorch documentation. If you have any questions, you can ask me (Natacha), it could help me improve these tutorials and / or help other students who are struggling as much as you are. \n",
    "\n",
    "These tutorials are a \"bonus\", they are not mandatory and are not graded (there is nothing to do anyway, just read and run). They are just here to help you if you are new to PyTorch and to help you save some time by not reading the book (or at least less intensively). \n",
    "\n",
    "In short: To understand deep learning concepts, the number one priority is Andrew's course. To understand PyTorch, the priority is the documentation (always), these tutorials and if it's still not enough, don't be afraid of trying to find good tutorials on the internet, there are plenty of them and you can share them with other students (and with us) if you find some really good ones.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Loading data \n",
    "\n",
    "  1.1 Loading CIFAR-10  (see previous tutorial)\n",
    "  1.2 From CIFAR-10 to CIFAR-2  \n",
    "\n",
    "2. Basic building blocks for neural networks in PyTorch  \n",
    "\n",
    "  2.1 The 'torch.nn' module and the 'torch.nn.Module' class  \n",
    "  2.2 Our network as a nn.Sequential object  \n",
    "  2.3 Inspecting a module object\n",
    "\n",
    "3. Training our model\n",
    "\n",
    "  3.1 Training on CPU  \n",
    "  3.2 Training on GPU \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f2c09d710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import datetime\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "### 1.1 Loading CIFAR-10  (see previous tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Size of the training dataset:  50000\n",
      "Files already downloaded and verified\n",
      "Size of the validation dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "# Where to find the data or where to download the data if not found\n",
    "data_path = 'data/'\n",
    "\n",
    "# Instantiates a dataset for the training data and downloads the data if it is not present\n",
    "cifar10_train = datasets.CIFAR10(\n",
    "    data_path,       # location from which the data will be downloaded\n",
    "    train=True,      # says whether weâ€™re interested in the training set or the validation set\n",
    "    download=True,   # says whether we allow PyTorch to download the data if not found in 'data_path'\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "print('Size of the training dataset: ', len(cifar10_train))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, \n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "print('Size of the validation dataset: ', len(cifar10_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 From CIFAR-10 to CIFAR-2\n",
    "\n",
    "We define a lighter version of CIFAR-10, which is now CIFAR-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training dataset:  10000\n",
      "Size of the validation dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2_train = [(img, label_map[label])\n",
    "          for img, label in cifar10_train\n",
    "          if label in [0, 2]]\n",
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]\n",
    "\n",
    "print('Size of the validation dataset: ', len(cifar2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic building blocks for neural networks in PyTorch \n",
    "\n",
    "### 2.1 The 'torch.nn' module and the 'torch.nn.Module' class\n",
    "\n",
    "In Pytorch, the basic building blocks for neural networks are available in the [torch.nn](https://pytorch.org/docs/stable/nn.html) module (often imported as 'nn')\n",
    "\n",
    "Then the base class for all the basic components of a neural network is the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "For example:\n",
    "\n",
    "- the [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) activation fonction is a subclass of the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class\n",
    "- the 1D convolutional layer [nn.Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d) is a subclass of the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class\n",
    "- the MSE loss function [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) is a subclass of the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class\n",
    "- the distance function [nn.PairwiseDistance ](https://pytorch.org/docs/stable/generated/torch.nn.PairwiseDistance.html#torch.nn.PairwiseDistance) is a subclass of the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class\n",
    "- the container [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) (will see in the next cell what it is exactly)  is also a subclass of the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class\n",
    "\n",
    "Exception: \n",
    "\n",
    "- [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) is not a subclass of [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) but of [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) instead (the other extremely important class in PyTorch)\n",
    "\n",
    "So in short, almost everything in torch.nn can be seen as a nn.Module in PyTorch :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things implemented in nn.module inherit from the nn.Module class\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "nn.Parameter is not a subclass of nn.Module but of torch.Tensor instead\n",
      "False\n",
      "True\n",
      "Things implemented outside nn module don't inherit from the nn.Module class\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"Things implemented in nn.module inherit from the nn.Module class\")\n",
    "print(issubclass(nn.ReLU, nn.Module))\n",
    "print(issubclass(nn.Conv1d, nn.Module))\n",
    "print(issubclass(nn.MSELoss, nn.Module))\n",
    "print(issubclass(nn.PairwiseDistance, nn.Module))\n",
    "print(issubclass(nn.Sequential, nn.Module))\n",
    "print(\"nn.Parameter is not a subclass of nn.Module but of torch.Tensor instead\")\n",
    "print(issubclass(nn.Parameter, nn.Module))\n",
    "print(issubclass(nn.Parameter, torch.Tensor))\n",
    "print(\"Things implemented outside nn module don't inherit from the nn.Module class\")\n",
    "print(issubclass(torchvision.transforms.Resize, nn.Module))\n",
    "print(issubclass(torch.Tensor, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2.2 Our network as a nn.Sequential object\n",
    "\n",
    "*(inspired by 6.3. Finally a neural network)*\n",
    "\n",
    "Now what is this \n",
    "*[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) container* thing? \n",
    "Well [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential) provides a simple way to concatenate [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 32*32*3   # Determined by our dataset: 32x32 RGB images\n",
    "n_hidden1 = 256  # Choose whatever you want here, often powers of 2\n",
    "n_hidden2 = 64\n",
    "n_out = 2        # Determined by our number of classes, so 2: birds and planes\n",
    "\n",
    "model_seq = nn.Sequential(\n",
    "    # Flatten is required in our case (and in many cases) because\n",
    "    # our input are (32x32x3) dimensional and nn layers \n",
    "    # expect 1D inputs\n",
    "    nn.Flatten(),                    \n",
    "    nn.Linear(n_in, n_hidden1),\n",
    "    nn.Tanh(),                       # Choose any activation function available in the nn module\n",
    "    nn.Linear(n_hidden1, n_hidden2), # Choose any layer available in the nn module\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_hidden2, n_out),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feeding one image to our custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6139, -0.3228, -0.1164,  ..., -0.2593, -0.2752, -0.5451],\n",
      "          [ 0.6615, -0.1482, -0.8467,  ..., -0.3228, -0.3228, -0.5768],\n",
      "          [ 0.2329,  0.2646, -0.1005,  ..., -0.3387, -0.6562, -0.7515],\n",
      "          ...,\n",
      "          [ 0.2170,  0.2646,  0.1535,  ..., -0.5768, -0.4498,  0.0106],\n",
      "          [ 0.5980,  0.4393,  0.3281,  ..., -0.6404, -0.4340,  0.0265],\n",
      "          [ 0.9156,  0.8044,  0.4551,  ..., -0.4975, -0.5451, -0.0529]],\n",
      "\n",
      "         [[ 1.3369,  0.2740,  0.4028,  ...,  0.3867,  0.3867,  0.0968],\n",
      "          [ 1.4497,  0.5961, -0.2253,  ...,  0.3062,  0.3062,  0.0646],\n",
      "          [ 1.0954,  1.1276,  0.6444,  ...,  0.2579, -0.0481, -0.1286],\n",
      "          ...,\n",
      "          [ 0.4028,  0.5156,  0.5317,  ...,  0.1774,  0.4028,  0.8538],\n",
      "          [ 0.5478,  0.6605,  0.6605,  ...,  0.1130,  0.4028,  0.8860],\n",
      "          [ 0.4834,  0.9504,  0.4995,  ...,  0.1774,  0.1613,  0.7572]],\n",
      "\n",
      "         [[-0.4487, -0.7935, -0.1939,  ..., -0.6136, -0.6736, -0.8535],\n",
      "          [-0.4487, -0.9734, -1.0634,  ..., -0.5237, -0.6286, -0.8535],\n",
      "          [-0.7336, -0.7186, -0.5237,  ..., -0.4487, -0.8385, -0.9734],\n",
      "          ...,\n",
      "          [-0.4937, -0.5986, -0.6286,  ..., -1.2882, -1.3482, -0.9734],\n",
      "          [-0.4337, -0.4787, -0.3588,  ..., -1.4231, -1.3032, -0.9884],\n",
      "          [-0.1789,  0.0310, -0.2089,  ..., -1.2732, -1.3182, -1.0484]]]])\n",
      "tensor([[-0.0510, -0.3430]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "batch_t = torch.unsqueeze(cifar2_train[0][0], 0)\n",
    "out = model_seq(batch_t)\n",
    "print(batch_t)\n",
    "print(out)   # out values are just rubbish since the nn is not trained yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inspecting a module object\n",
    "\n",
    "We saw earlier that the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is an essential part of the PyTorch library and that it is the base class for all the basic components of a neural network.\n",
    "The fact that so many PyTorch objects inherit from this class has many advantages. One oh them is that they share many important methods such as:\n",
    "\n",
    "- [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward) Defines the computation performed at every call. **Should be overridden by all subclasses** (We'll see that later)\n",
    "- [modules](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.modules): Returns an iterator over all modules in the network.\n",
    "- [named\\_modules](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_modules): Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
    "- [parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters): Returns an iterator over module parameters (the so called [nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html), the subclass of Tensor)\n",
    "- [named\\_parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters): Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
    "- [requires\\_grad\\_](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.requires_grad_) Change if autograd should record operations on parameters in this module. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).\n",
    "- [state_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict): Returns a dictionary containing a whole state of the module\n",
    "- [load\\_state\\_dict](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict): Copies parameters and buffers from state_dict into this module and its descendants\n",
    "- [to](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to) Moves and/or casts the parameters and buffers (typically to a GPU or CPU)\n",
    "- [cpu](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu) / [cuda](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda): Moves all model parameters and buffers to the CPU / GPU\n",
    "- [train](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train) / [eval](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval): Sets the module in training/evaluation mode\n",
    "- [zero_grad](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad): Sets gradients of all model parameters to zero.\n",
    "\n",
    "We will use most of these method in this tutorial already, and let's start with the ones returning parameters / modules.\n",
    "\n",
    "[torch.numel](https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel) returns the total number of elements in a given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting parameters\n",
      "name:  1.weight    length:  786432\n",
      "name:  1.bias    length:  256\n",
      "name:  3.weight    length:  16384\n",
      "name:  3.bias    length:  64\n",
      "name:  5.weight    length:  128\n",
      "name:  5.bias    length:  2\n",
      "[786432, 256, 16384, 64, 128, 2]\n",
      "Total number of parameters:  803266\n",
      "\n",
      "Inspecting modules\n",
      "('', Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (2): Tanh()\n",
      "  (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=64, out_features=2, bias=True)\n",
      "))\n",
      "('0', Flatten())\n",
      "('1', Linear(in_features=3072, out_features=256, bias=True))\n",
      "('2', Tanh())\n",
      "('3', Linear(in_features=256, out_features=64, bias=True))\n",
      "('4', ReLU())\n",
      "('5', Linear(in_features=64, out_features=2, bias=True))\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspecting parameters\")\n",
    "for p in model_seq.named_parameters():\n",
    "    print(\"name: \", p[0], \"   length: \", p[1].numel())\n",
    "print([p.numel() for p in model_seq.parameters() if p.requires_grad == True])\n",
    "print(\"Total number of parameters: \", sum([p.numel() for p in model_seq.parameters()]))\n",
    "\n",
    "print(\"\\nInspecting modules\")\n",
    "for m in model_seq.named_modules():\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training our model\n",
    "\n",
    "### 3.1 Training on CPU\n",
    "\n",
    "#### Defining the training loop \n",
    "\n",
    "*(inspired by 8.4 Training our convnet)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        loss_train = 0\n",
    "\n",
    "        # Loop over our dataset in the batches the data loader creates for us\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            # Feed a batch through our model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we wish to minimize\n",
    "            loss = loss_fn(outputs, labels) \n",
    "            \n",
    "            # Get rid of the gradients from the last round\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Perform the backward step. That is, compute the gradients of all parameters we want the network to learn\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updates the model\n",
    "            optimizer.step() \n",
    "\n",
    "            # Sums the losses we saw over the epoch.\n",
    "            # Recall that it is important to transform the loss to a Python number with .item()\n",
    "            loss_train += loss.item() \n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using the training loop\n",
    "\n",
    "On my computer training 10 epochs takes 20 secondes (we will compare this with the gpu version of the training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-08 10:54:53.259972 Epoch 1, Training loss 0.5393446640224214\n",
      "2021-02-08 10:55:04.230352 Epoch 10, Training loss 0.33766555340047094\n",
      "2021-02-08 10:55:14.297124 Epoch 20, Training loss 0.23722393321953003\n"
     ]
    }
   ],
   "source": [
    "# The DataLoader batches up the examples of our cifar2 dataset\n",
    "# Here we use shuffle = True to shuffle the dataset for the training\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True) \n",
    "\n",
    "# Instantiate the optimizer, here:\n",
    "# 1. Stochastic Gradient Descent optimizer, \n",
    "# 2. that has to be applied to our parameters (model.parameters())\n",
    "# 3. With a learning rate of 1e-2\n",
    "optimizer = optim.SGD(model_seq.parameters(), lr=1e-2)\n",
    "\n",
    "# Instantiate the loss function (here we use cross entropy)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Now all we have to do is calling the training loop\n",
    "# WARNING THIS MIGHT BE EXTREMELY SLOW. STOP YOUR KERNEL TO STOP THE TRAINING\n",
    "# DON'T TRY TO LET IT FINISH WE WILL SEE HOW TO USE GPUs AT THE END OF THIS NOTEBOOK\n",
    "training_loop(\n",
    "    n_epochs = 21,\n",
    "    optimizer = optimizer,\n",
    "    model = model_seq,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.92\n",
      "Accuracy val: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Here we use shuffle = False\n",
    "# Because it is easier to check the predictions made.\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # We do not want gradients here, as we will not want to update the parameters.\n",
    "        with torch.no_grad(): \n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) \n",
    "                total += labels.shape[0] \n",
    "                correct += int((predicted == labels).sum()) \n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model_seq, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training on GPU\n",
    "\n",
    "*(Inspired by 8.4.3 Training on the GPU)*\n",
    "\n",
    "#### Check if a GPU is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_on_gpu(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            # These two lines following lines are what differs from \n",
    "            # our previous training_loop function.\n",
    "            # They move imgs and labels to the device we are training\n",
    "            # on (gpu if available, cpu otherwise)\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model using the training loop\n",
    "\n",
    "On my computer training 10 epochs takes now only 3 seconds, so it's around 7 times faster than using the cpu! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-08 10:55:18.710844 Epoch 1, Training loss 0.21863427715506523\n",
      "2021-02-08 10:55:21.117956 Epoch 10, Training loss 0.13799390324931235\n",
      "2021-02-08 10:55:23.789728 Epoch 20, Training loss 0.07704896918812375\n",
      "2021-02-08 10:55:26.571278 Epoch 30, Training loss 0.05107200264361254\n",
      "2021-02-08 10:55:29.385551 Epoch 40, Training loss 0.021427364219098712\n",
      "2021-02-08 10:55:32.053173 Epoch 50, Training loss 0.013399796317195057\n",
      "2021-02-08 10:55:34.861965 Epoch 60, Training loss 0.00854919287880325\n",
      "2021-02-08 10:55:38.069740 Epoch 70, Training loss 0.005819924617672612\n",
      "2021-02-08 10:55:41.179055 Epoch 80, Training loss 0.004303469106104154\n",
      "2021-02-08 10:55:44.255177 Epoch 90, Training loss 0.003382411909995565\n",
      "2021-02-08 10:55:47.308105 Epoch 100, Training loss 0.0028218572935575894\n"
     ]
    }
   ],
   "source": [
    "# Again shuffle = True for the training phase\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Moves our model (all parameters) to the GPU. If \n",
    "# you forget to move either the model or the inputs to the\n",
    "# GPU, you will get errors about tensors not being on the same\n",
    "# device, because the PyTorch operators do not support\n",
    "# mixing GPU and CPU inputs.\n",
    "model_seq.to(device=device) \n",
    "optimizer = optim.SGD(model_seq.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# WARNING THIS IS SUPPOSED TO MUCH MUCH FASTER THAN PREVIOUSLY BUT IT MIGHT STILL\n",
    "# TAKE A WHILE IF:\n",
    "#  - YOUR GPU IS NOT AVAILABLE\n",
    "#  - YOUR GPU IS NOT THE BEST GPU EVER. (Trying not to hurt your GPU's feeling here :) )\n",
    "# AGAIN STOP YOUR KERNEL IF IT'S TOO SLOW \n",
    "training_loop_on_gpu(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model_seq,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Again shuffle = False for the validation phase\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "def validate_on_gpu(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                # These two lines following lines are what differs from \n",
    "                # our previous validate function.\n",
    "                # They move imgs and labels to the device we are predicting\n",
    "                # on (gpu if available, cpu otherwise)\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate_on_gpu(model_seq, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
