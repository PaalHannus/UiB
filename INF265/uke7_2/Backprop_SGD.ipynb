{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to computational graph and implementation of backpropagation and Gradient descent\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "\n",
    "Each week you will be given an assignment related to the associated module. You have roughly one week to complete and submit each of them. There are 3 weekly group sessions available to help you complete the assignments. Attendance is not mandatory but recommended. However, **assignments are graded each week and not submitting them or submitting them after the deadline will give you no points**\n",
    "\n",
    "- **FORMAT**: Jupyter notebook\n",
    "- **DEADLINE**:  Sunday 21st February, 23:59\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Last week we saw how adding regularization to SGD could reduce overfitting. This week we will try to understand what happens exactly inside your model during the training process when applying the gradient descent method, with or without regularization and momentum. To do so we will simply implement first the backprogation algorithm which is how we compute the gradients and then the gradient descent, which is the trainable parameter update in response to this gradient.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Utils  \n",
    "\n",
    "2. MiniNet  \n",
    "\n",
    "3. Introduction to PyTorch's computational graph  \n",
    "  3.1 Weight values and update  \n",
    "  3.2 Gentle (but still painful) introduction to computational graph  \n",
    "4. From an optimization problem to the backprogation algorithm  \n",
    "\n",
    "5. Implementing Gradient descent in Pytorch inside the training loop   \n",
    "  5.0 LetNet5  \n",
    "  5.1 Manual weight optimization  \n",
    "  5.2 Compare results to optim.SGD: vanilla version  \n",
    "  5.3 Compare results to optim.SGD: (with Regularization)  \n",
    "  5.4 Compare results to optim.SGD: (with Momentum)  \n",
    "  5.5 Compare results to optim.SGD: (with Regularization and Momentum)  \n",
    "6. Implementing an Optimizer in PyTorch\n",
    "  \n",
    "## Andrew's Videos related to today's assignent (entirely optional! just here to help you if you're confused about something specific!)\n",
    "\n",
    "- [Computation Graph (C1W2L07)](https://www.youtube.com/watch?v=hCP1vGoCdYU&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=13)\n",
    "- [Derivatives With Computation Graphs (C1W2L08)](https://www.youtube.com/watch?v=nJyUyKN-XBQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=14)\n",
    "- [Gradient Descent (C1W2L04)](https://www.youtube.com/watch?v=uJryes5Vk1o&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=10)\n",
    "- [Gradient Descent For Neural Networks (C1W3L09)](https://www.youtube.com/watch?v=7bLEWDZng_M&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=33) (includes backprogation)\n",
    "- [Backpropagation Intuition (C1W3L10)](https://www.youtube.com/watch?v=yXcQ4B-YSjQ&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=34)\n",
    "- [Deep L-Layer Neural Network (C1W4L01)](https://www.youtube.com/watch?v=2gw5tE2ziqA&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=36) (clarify notations)\n",
    "- [Forward and Backward Propagation (C1W4L06)](https://www.youtube.com/watch?v=qzPQ8cEsVK8&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=41)\n",
    "- [Regularization (C2W1L04)](https://www.youtube.com/watch?v=6g0t3Phly2M&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=4)\n",
    "- [Why Regularization Reduces Overfitting (C2W1L05)](https://www.youtube.com/watch?v=NyG-7nRpsW8&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=5)\n",
    "- [Gradient Descent With Momentum (C2W2L06)](https://www.youtube.com/watch?v=k8fTYJPd3_I&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import datetime\n",
    "import copy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utils\n",
    "\n",
    "Nothing to see in the cell below, just the definition of 3 functions we'll need later, you don't even need to read them, just know that there is:\n",
    "\n",
    "- ``load_cifar``: load CIFAR-2 (keeping only birds and plane)\n",
    "- ``training_loop``\n",
    "- ``validate``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of the training dataset:  10000\n",
      "Size of the validation dataset:  2000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "\n",
    "def load_cifar(data_path='../data/'):\n",
    "    \"\"\"\n",
    "    Return CIFAR-2 keeping only birds and plane\n",
    "    \"\"\"\n",
    "    cifar10_train = datasets.CIFAR10(\n",
    "        data_path,       \n",
    "        train=True,      \n",
    "        download=True,   \n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ]))\n",
    "\n",
    "    cifar10_val = datasets.CIFAR10(\n",
    "        data_path, \n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                (0.2470, 0.2435, 0.2616))\n",
    "        ]))\n",
    "\n",
    "    label_map = {0: 0, 2: 1}\n",
    "    class_names = ['airplane', 'bird']\n",
    "\n",
    "    cifar2_train = [(img, label_map[label]) for img, label in cifar10_train if label in [0, 2]]\n",
    "    cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n",
    "    print('Size of the training dataset: ', len(cifar2_train))\n",
    "    print('Size of the validation dataset: ', len(cifar2_val))\n",
    "\n",
    "    return cifar2_train, cifar2_val\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \"\"\"\n",
    "    Train our model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "cifar2_train, cifar2_val = load_cifar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MiniNet\n",
    "\n",
    "Here is the definition of a very simple MLP that we will use in the first part of this assignment. It corresponds to the following network (I tried to follow Andrew's notation):\n",
    "\n",
    "![MiniNet architecture](MiniNet.jpeg)\n",
    "\n",
    "As you will use this implementation, it might be useful to read the code bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=3)\n",
    "        self.fc2 = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "        # Where we will store our neuron values\n",
    "        # - z: before activation function \n",
    "        # - a: after activation function (a=f(z))\n",
    "        self.z = {str(i) : None for i in range(1,3)}\n",
    "        self.a = {str(i) : None for i in range(3)}\n",
    "\n",
    "        # Where we will store the gradients for our custom backpropagation algo\n",
    "        self.dL_dw = {str(i) : None for i in range(1,3)}\n",
    "        self.dL_db = {str(i) : None for i in range(1,3)}\n",
    "\n",
    "        # Derivatives of our activation functions\n",
    "        self.df = {str(i) : None for i in range(1,3)}\n",
    "        self.df['1'] = lambda x : torch.div(1, torch.cosh(x)**2)\n",
    "        self.df['2'] = lambda x : torch.div(1, torch.cosh(x)**2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The first dimension of the input must be the batch size\n",
    "        out = x.view(-1, 2)\n",
    "\n",
    "        # Input layer\n",
    "        self.a['0'] = out\n",
    "        \n",
    "        # First layer (hidden layer)\n",
    "        self.z['1'] = self.fc1(out)\n",
    "        self.a['1'] = torch.tanh(self.z['1'])\n",
    "        \n",
    "        # Second layer (output layer)\n",
    "        self.z['2'] = self.fc2(self.a['1'])\n",
    "        self.a['2'] = torch.tanh(self.z['2'])\n",
    "\n",
    "        return self.a['2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Introduction to PyTorch's computational graph\n",
    "\n",
    "### 3.1 Weight values and update\n",
    "\n",
    "Get more familiar with models in Pytorch. Look at the cell below to understand how to:\n",
    "\n",
    "- Access neuron values\n",
    "- Access parameter values\n",
    "- Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ============== Parameters ============== \n",
      "\n",
      "Name :  fc1.weight \n",
      "Value:  tensor([[ 0.1244,  0.3575],\n",
      "        [ 0.3765, -0.5189],\n",
      "        [-0.5111, -0.0669]])\n",
      "\n",
      "Name :  fc1.bias \n",
      "Value:  tensor([-0.6967, -0.1077, -0.6680])\n",
      "\n",
      "Name :  fc2.weight \n",
      "Value:  tensor([[ 0.2316, -0.2954, -0.5686],\n",
      "        [-0.5067,  0.3854, -0.2325]])\n",
      "\n",
      "Name :  fc2.bias \n",
      "Value:  tensor([0.5639, 0.0136])\n",
      "\n",
      " ========= Print state_dict() ========= \n",
      "OrderedDict([('fc1.weight', tensor([[ 0.1244,  0.3575],\n",
      "        [ 0.3765, -0.5189],\n",
      "        [-0.5111, -0.0669]])), ('fc1.bias', tensor([-0.6967, -0.1077, -0.6680])), ('fc2.weight', tensor([[ 0.2316, -0.2954, -0.5686],\n",
      "        [-0.5067,  0.3854, -0.2325]])), ('fc2.bias', tensor([0.5639, 0.0136]))])\n",
      "\n",
      " ========== Updated parameters ==========\n",
      "Parameter containing:\n",
      "tensor([[42.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([42.,  1.], requires_grad=True)\n",
      "\n",
      " ============== Neuron values ============== \n",
      "\n",
      " -------------- Input ---------------- \n",
      "a0:             None\n",
      "\n",
      " -------------- First Layer ---------------- \n",
      "z1:             None\n",
      "a1 = tanh(z1):  None\n",
      "\n",
      " --------------  2nd Layer  ---------------- \n",
      "z2:             None\n",
      "a2 = tanh(z2):  None\n",
      "\n",
      " ============== Neuron values ============== \n",
      "\n",
      " -------------- Input ---------------- \n",
      "a0:             tensor([[1., 1.]])\n",
      "\n",
      " -------------- First Layer ---------------- \n",
      "z1:             tensor([[42.3033,  1.8923,  1.3320]], grad_fn=<AddmmBackward>)\n",
      "a1 = tanh(z1):  tensor([[1.0000, 0.9556, 0.8697]], grad_fn=<TanhBackward>)\n",
      "\n",
      " --------------  2nd Layer  ---------------- \n",
      "z2:             tensor([[41.4548,  0.6594]], grad_fn=<AddmmBackward>)\n",
      "a2 = tanh(z2):  tensor([[1.0000, 0.5780]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = MiniNet()\n",
    "\n",
    "def print_parameters(model):\n",
    "    \"\"\"\n",
    "    Print trainable parameters of our MiniNet\n",
    "    \"\"\"\n",
    "    print(\" ============== Parameters ============== \")\n",
    "    for name, p in model.named_parameters():\n",
    "        print(\"\\nName : \", name, \"\\nValue: \", p.data)\n",
    "\n",
    "def print_neuron_values(model):\n",
    "    \"\"\"\n",
    "    Print neuron values (a and z) of our MiniNet \n",
    "    \"\"\"\n",
    "    print(\"\\n ============== Neuron values ============== \")\n",
    "    print(\"\\n -------------- Input ---------------- \")\n",
    "    print(\"a0:            \", model.a['0'] )\n",
    "    print(\"\\n -------------- First Layer ---------------- \")\n",
    "    print(\"z1:            \", model.z['1'] )\n",
    "    print(\"a1 = tanh(z1): \", model.a['1'] )\n",
    "    print(\"\\n --------------  2nd Layer  ---------------- \")\n",
    "    print(\"z2:            \", model.z['2'] )\n",
    "    print(\"a2 = tanh(z2): \", model.a['2'] )\n",
    "\n",
    "\n",
    "print_parameters(model) # We can see that all parameters are randomly initialized\n",
    "\n",
    "# We can access the different parameters using their names as specified in model.state_dict()\n",
    "print(\"\\n ========= Print state_dict() ========= \")\n",
    "print(model.state_dict())\n",
    "# We can now access one of our parameters using its name\n",
    "# And we can update its value using '.data' after its name\n",
    "model.fc1.weight.data = torch.ones(3,2)\n",
    "model.fc1.weight.data[0,0] = 42\n",
    "\n",
    "model.fc2.bias.data[:] = torch.ones(2)\n",
    "model.fc2.bias.data[0] = 42\n",
    "\n",
    "print(\"\\n ========== Updated parameters ==========\" )\n",
    "\n",
    "print(model.fc1.weight)\n",
    "print(model.fc2.bias)\n",
    "\n",
    "\n",
    "# We have not given any input to our model yet, so all neuron values should be None\n",
    "print_neuron_values(model)\n",
    "# Now we give some input...\n",
    "input = torch.Tensor([1, 1])\n",
    "model(input)\n",
    "# ... and everything has been computed in the forward pass \n",
    "print_neuron_values(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gentle (but still painful) introduction to computational graph\n",
    "\n",
    "If you take a closer look at the output of \"Updated parameters\" you can see that it mentions \"``requires_grad=True``\". Then, if you take a closer look at the output of \"neuron values\" you see that it mentions \"``grad_fn=<TanhBackward>``\"  \"``grad_fn=<AddmmBackward>``\"\n",
    "\n",
    "What is this all about? Well, it has to do with the *computational graph* which is how Pytorch manages all the operations made during the forward pass (``outputs = model(inputs)`` i.e ``forward`` method) so that it can compute all the gradients in the backward pass (``loss.backward()``) and finally update parameters accordingly when calling ``optimizer.step()``. \n",
    "\n",
    "Now to illustrate how necessary it is to have some understanding of this computational graph, let's try to initialize our weights in a custom way and check how easily we can mess up things.\n",
    "\n",
    "This is okay:\n",
    "\n",
    "- ``model.layer.param.data = new_values``\n",
    "- ``model.layer.param.data[:] = new_values``\n",
    "\n",
    "This is **NOT** okay:\n",
    "\n",
    "- ``model.layer.param = new_values``     Raises an error (unless ``new_values`` are of nn.Parameter and not ``torch.Tensor``): ``TypeError: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)``\n",
    "- ``model.layer.param[:] = new_values``  Will remove the parameter from the list of leaves and put ``CopySlices`` as gradient function\n",
    "\n",
    "Basically, we want each of our trainable parameters (weights) to require grad ([requires_grad](https://pytorch.org/docs/stable/autograd.html?highlight=requires#torch.Tensor.requires_grad)) and to be a leaf ([is_leaf](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.is_leaf)). Variables that have nothing to do with the computational graph (not a part of the network) should be detached of the computational graph. (see [detach](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach) or [detach_](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach_) for in place version). \n",
    "\n",
    "The concept of leaf might be counter intuitive in PyTorch. The fact that your weight is in the middle of your network does not mean that it should not be a leaf. It should always be a leaf. Basically weights are leaves because in the forward pass their values do not depend on the values of the input nor the weights, etc. Their values only change when you call optimizer.step() or when you initialize them manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All parameters are correctly attached to the computational graph! :) \n",
      " ============== Parameters ============== \n",
      "\n",
      "Name :  fc1.weight \n",
      "Value:  tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.4000],\n",
      "        [0.5000, 0.6000]])\n",
      "\n",
      "Name :  fc1.bias \n",
      "Value:  tensor([0.7000, 0.8000, 0.9000])\n",
      "\n",
      "Name :  fc2.weight \n",
      "Value:  tensor([[1.0000, 1.1000, 1.2000],\n",
      "        [1.3000, 1.4000, 1.5000]])\n",
      "\n",
      "Name :  fc2.bias \n",
      "Value:  tensor([1.6000, 1.7000])\n",
      "\n",
      "All parameters are correctly attached to the computational graph! :) \n",
      " ==== WARNING ====  fc1.weight is not a leaf!\n",
      "Parameter containing:\n",
      "tensor([[0.1000, 0.2000],\n",
      "        [0.3000, 0.4000],\n",
      "        [0.5000, 0.6000]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def weight_initialization(model):\n",
    "    \"\"\"\n",
    "    Initialize our MiniNet's weights\n",
    "    \"\"\"\n",
    "    model.fc1.weight.data = torch.arange(1,7, dtype=torch.float).view(3,2)/10\n",
    "    model.fc1.bias.data = torch.arange(7,10, dtype=torch.float)/10\n",
    "    model.fc2.weight.data = torch.arange(10,16, dtype=torch.float).view(2,3)/10\n",
    "    model.fc2.bias.data = torch.arange(16,18, dtype=torch.float)/10\n",
    "    return model\n",
    "\n",
    "def check_computational_graph(model):\n",
    "    \"\"\"\n",
    "    Make sure all trainable parameters require grad and are leaves\n",
    "    \"\"\"\n",
    "    res = True\n",
    "    # Go through layer 1,2\n",
    "    for i_layer in range(1,3):\n",
    "        # Each layer has a weight and bias parameter\n",
    "        for param_name in ['weight', 'bias']:\n",
    "            \n",
    "            layer_name = \"fc\"+str(i_layer)\n",
    "            # 'getattr(object, string variable)' is like `object.myattribut` when variable = \"myattribute\"\n",
    "            if not getattr(getattr(model, layer_name), param_name).requires_grad:\n",
    "                print(\" ==== WARNING ==== \", layer_name + \".\" + param_name + \"does not require grad!\")\n",
    "                print(getattr(getattr(model, layer_name), param_name))\n",
    "                res = False\n",
    "            if not getattr(getattr(model, layer_name), param_name).is_leaf:\n",
    "                print(\" ==== WARNING ==== \", layer_name + \".\" + param_name + \" is not a leaf!\")\n",
    "                print(getattr(getattr(model, layer_name), param_name))\n",
    "                res = False\n",
    "    if res:\n",
    "        print(\"\\nAll parameters are correctly attached to the computational graph! :) \")\n",
    "\n",
    "\n",
    "model = MiniNet()\n",
    "check_computational_graph(model)      # So far so good, since we have not done anything yet\n",
    "model = weight_initialization(model)\n",
    "print_parameters(model)               # To check that parameters have been updated\n",
    "check_computational_graph(model)      # To check that there are still correctly attached to the graph\n",
    "model.fc1.weight[:,:] = torch.arange(1,7, dtype=torch.float).view(3,2)/10\n",
    "check_computational_graph(model)      # Now fc1.weight is not a leaf anymore! (and see \"grad_fn=<CopySlices>\"\")\n",
    "\n",
    "# This would raise an error if uncommented\n",
    "#model.fc1.weight=torch.arange(1,7, dtype=torch.float).view(3,2)/10 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From an optimization problem to the backprogation algorithm\n",
    "\n",
    "** Optimization problem**\n",
    "\n",
    "In optimization, (so not only specific to machine learning) the problem often boils down to minimize (or maximize) some loss function (or score). The loss function can for instance be the mean squared error:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{m}\\sum_{s=1}^{m}\\frac{1}{2}||\\mathbf{y_s} - \\mathbf{\\hat{y}_s}||^2_2$$ \n",
    "\n",
    "with:\n",
    "\n",
    "- $m$ total number of sample in your dataset (or batch)\n",
    "- $\\theta$ all the parameters to be optimized ($\\in \\mathrm{R}^q$)\n",
    "- $\\mathbf{y}$ your expected result ($\\in \\mathrm{R}^{n \\times m}$)\n",
    "- $\\mathbf{\\hat{y}}$ your predicted result ($\\in \\mathrm{R}^{n \\times m}$)\n",
    "- $L : \\mathrm{R}^q \\rightarrow \\mathrm{R}$ your loss function\n",
    "\n",
    "**Gradient descent**\n",
    "\n",
    "One way to solve this problem (still not only specific to machine learning) is to minimize the loss function iteratively using the gradient descent method. That is to say, we iteratively update all the parameters as follows (in simplest version of the gradient descent algorithm):\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\nabla L(\\theta)  $$\n",
    "\n",
    "with \n",
    "- $\\alpha$ often called \"step\" in optimization and \"learning rate\" in machine learning\n",
    "- $\\nabla L(\\theta)$ the gradient of the loss function \n",
    "\n",
    "So if $\\theta = \\begin{bmatrix} w_{1} \\cdots w_{q} \\end{bmatrix}^T$  then $\\nabla L(\\theta) = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_{1}}(\\theta) \\cdots \\frac{\\partial L}{\\partial w_{q}}(\\theta) \\end{bmatrix}^T$\n",
    "\n",
    "**Backpropagation algorithm**\n",
    "\n",
    "So the only thing we need is to compute $\\frac{\\partial L}{\\partial w_{i}}, \\forall i \\in [1..q]$. In machine learning, we use the backpropagation algorithm to compute $\\frac{\\partial L}{\\partial w_{i}}$ for all the weights $w_{i}$ of a neural network. This is a recursive algorithm, from the output layer to the input layer (hence the name *backpropagation*). Again, I tried to follow Andrew's notation.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{[l]}_{i,j}} = \\delta^{[l]}_i \\times a_j^{[l-1]} \\qquad  \\qquad \\forall l \\in [1 .. L] \\quad \\text{with }  \\delta^{[l]}_i \\text{ called \"local gradient\" }\\qquad \\delta^{[l]}_i = \\frac{\\partial L}{\\partial z^{[l]}_{i}} = \\frac{\\partial L}{\\partial a^{[l]}_{i}} \\times \\frac{\\partial a^{[l]}_{i}}{\\partial z^{[l]}_{i}}$$\n",
    "\n",
    "For the output layer, since $a^{[L]}_{i} = \\hat{y}_{i}$:\n",
    "\n",
    "$$\\delta^{[L]}_i = \\frac{\\partial L}{\\partial z^{[L]}_{i}} = \\frac{\\partial L}{\\partial \\hat{y}_{i}} \\times \\frac{\\partial \\hat{y}_i}{\\partial z^{[L]}_{i}} = e_i'(\\hat{y}_i) \\times f'^{[L]}_i (z^{[L]}_{i})$$\n",
    "\n",
    "For the hidden layers (general case):\n",
    "\n",
    "$$\\delta^{[l]}_i = \\frac{\\partial L}{\\partial z^{[l]}_{i}} = \\frac{\\partial L}{\\partial a^{[l]}_{i}} \\times \\frac{\\partial a^{[l]}_{i}}{\\partial z^{[l]}_{i}} = \\Big( \\sum_{k=1}^{n^{[l+1]}} \\frac{\\partial a^{[l]}_{i}}{\\partial a^{[l+1]}_{k}} \\frac{\\partial a^{[l+1]}_{k}}{\\partial a^{[l]}_{i}} \\Big)\\times \\frac{\\partial a^{[l]}_{i}}{\\partial z^{[l]}_{i}} = \\Big( \\sum_{k=1}^{n^{[l+1]}} \\delta_k^{[l+1]} w^{[l+1]}_{k,i} \\Big) \\times f'^{[l]}_i (z^{[l]}_{i})$$\n",
    "\n",
    "## TODO:\n",
    "\n",
    "Write a function ``backpropagation`` that computes:\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial w^{[l]}_{i,j}}$ and store them in ``model.dL_dw[str(l)][i,j]`` for $l \\in [1,2]$ \n",
    "- $\\frac{\\partial L}{\\partial b^{[l]}_{j}}$ and store them in ``model.dL_db[str(l)][j]`` for $l \\in [1,2]$ \n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Look at how MiniNet is implemented and at the cell above (``weight_initialization`` and ``check_computational_graph`` to see how to access MiniNet weights and update MiniNet attributes)\n",
    "\n",
    "Once you think you are done:\n",
    "\n",
    "- You can check if your function works well by runnning the next cell.\n",
    "- Since ``dL_dw`` are not trainable parameters of your model they should not be in the computational graph. Check that all your ``dL_dw``  (or at least one of them) have its attribute ``require_grad`` set to ``False``. If not use the [detach](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach) or [detach_](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach_) methods appropriately in your ``backpropagation`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return torch.sum((y_true - y_pred)**2)/2/len(y_true)\n",
    "\n",
    "def dmse(y_true, y_pred):\n",
    "    return torch.sum(-(y_true - y_pred))/len(y_true)\n",
    "\n",
    "\n",
    "def backpropagation(model, y_true, y_pred, dloss=dmse):\n",
    "# The first 'L' is the number layer and in dL_dw 'L' means 'Loss'\n",
    "    L = len(model.dL_dw)   \n",
    "    # ------------ Special case: output layer ------------ \n",
    "    # n(L)-array\n",
    "    delta_curr = dloss(y_true, y_pred) * model.df[str(L)](model.z[str(L)].data).data\n",
    "    # n(L)-n(L-1) array\n",
    "    model.dL_dw[str(L)] = delta_curr.view(-1, 1) * model.a[str(L-1)].data\n",
    "    model.dL_dw[str(L)].detach_()\n",
    "    # n(L) array\n",
    "    model.dL_db[str(L)] = delta_curr\n",
    "    model.dL_db[str(L)].detach_()\n",
    "    # ------------ General case: hidden layers ------------\n",
    "    for l in range(L-1, 0,-1):\n",
    "        #n(l+1)-array\n",
    "        delta_prev = delta_curr\n",
    "        # Compute all dai_dak * dak_dai, stacking results\n",
    "        # n(l+1) * n(l) array\n",
    "        layer_name = 'fc' + str(l+1)\n",
    "        tmp = delta_prev.view(-1, 1) * getattr(model, layer_name).weight.data\n",
    "        delta_curr = torch.sum(tmp, dim=0, keepdim=True)* model.df[str(l)](model.z[str(l)].data).data\n",
    "        model.dL_dw[str(l)] = delta_curr.view(-1, 1) * model.a[str(l-1)].data\n",
    "        model.dL_dw[str(l)].detach_()\n",
    "        model.dL_db[str(l)] = delta_curr\n",
    "        model.dL_db[str(l)].detach_()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " === Input 0 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.2290, -0.8101],\n",
      "        [ 0.2662, -0.9415],\n",
      "        [ 0.2665, -0.9427]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.2289, -0.8098],\n",
      "        [ 0.2661, -0.9413],\n",
      "        [ 0.2664, -0.9425]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 1 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 2.3832e-08, -6.0867e-08],\n",
      "        [ 1.0557e-08, -2.6963e-08],\n",
      "        [ 4.4003e-09, -1.1238e-08]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 1.6044e-08, -4.0975e-08],\n",
      "        [ 7.0864e-09, -1.8098e-08],\n",
      "        [ 2.9465e-09, -7.5253e-09]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 2 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 9.7754e-05, -2.9800e-04],\n",
      "        [ 2.9653e-05, -9.0394e-05],\n",
      "        [ 8.5141e-06, -2.5955e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 9.7749e-05, -2.9798e-04],\n",
      "        [ 2.9651e-05, -9.0390e-05],\n",
      "        [ 8.5137e-06, -2.5953e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 3 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0008, -0.0011],\n",
      "        [ 0.0005, -0.0007],\n",
      "        [ 0.0003, -0.0004]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0008, -0.0011],\n",
      "        [ 0.0005, -0.0007],\n",
      "        [ 0.0003, -0.0004]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 4 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0006, -0.0013],\n",
      "        [ 0.0003, -0.0007],\n",
      "        [ 0.0001, -0.0003]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0006, -0.0013],\n",
      "        [ 0.0003, -0.0007],\n",
      "        [ 0.0001, -0.0003]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 5 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0019, -0.0023],\n",
      "        [ 0.0013, -0.0015],\n",
      "        [ 0.0009, -0.0010]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0019, -0.0023],\n",
      "        [ 0.0013, -0.0015],\n",
      "        [ 0.0009, -0.0010]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 6 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0011, -0.0015],\n",
      "        [ 0.0005, -0.0007],\n",
      "        [ 0.0002, -0.0003]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0011, -0.0015],\n",
      "        [ 0.0005, -0.0007],\n",
      "        [ 0.0002, -0.0003]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 7 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 2.6015, -1.2535],\n",
      "        [ 4.3647, -2.1031],\n",
      "        [ 4.4339, -2.1364]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 2.6012, -1.2534],\n",
      "        [ 4.3641, -2.1028],\n",
      "        [ 4.4333, -2.1361]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 8 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0023, -0.0021],\n",
      "        [ 0.0018, -0.0016],\n",
      "        [ 0.0022, -0.0020]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0023, -0.0021],\n",
      "        [ 0.0018, -0.0016],\n",
      "        [ 0.0022, -0.0020]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 9 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 7.8193e-04, -9.9100e-04],\n",
      "        [ 2.1474e-04, -2.7215e-04],\n",
      "        [ 8.7939e-05, -1.1145e-04]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 7.8177e-04, -9.9081e-04],\n",
      "        [ 2.1469e-04, -2.7210e-04],\n",
      "        [ 8.7922e-05, -1.1143e-04]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 10 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0065, -0.0049],\n",
      "        [ 0.0096, -0.0072],\n",
      "        [ 0.0221, -0.0166]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0065, -0.0049],\n",
      "        [ 0.0096, -0.0072],\n",
      "        [ 0.0221, -0.0166]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 11 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 3.9924e-04, -4.3632e-04],\n",
      "        [ 1.0403e-04, -1.1369e-04],\n",
      "        [ 5.5116e-05, -6.0236e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 3.9927e-04, -4.3635e-04],\n",
      "        [ 1.0404e-04, -1.1370e-04],\n",
      "        [ 5.5122e-05, -6.0243e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 12 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0007, -0.0007],\n",
      "        [ 0.0002, -0.0002],\n",
      "        [ 0.0001, -0.0001]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0007, -0.0007],\n",
      "        [ 0.0002, -0.0002],\n",
      "        [ 0.0001, -0.0001]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 13 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0014, -0.0013],\n",
      "        [ 0.0011, -0.0010],\n",
      "        [ 0.0016, -0.0014]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0014, -0.0013],\n",
      "        [ 0.0011, -0.0010],\n",
      "        [ 0.0016, -0.0014]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 14 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 4.4950e-04, -5.1725e-04],\n",
      "        [ 9.2499e-05, -1.0644e-04],\n",
      "        [ 3.7774e-05, -4.3467e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 4.4960e-04, -5.1736e-04],\n",
      "        [ 9.2523e-05, -1.0647e-04],\n",
      "        [ 3.7781e-05, -4.3475e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 15 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0039, -0.0029],\n",
      "        [ 0.0073, -0.0055],\n",
      "        [ 0.0282, -0.0213]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0039, -0.0029],\n",
      "        [ 0.0073, -0.0055],\n",
      "        [ 0.0282, -0.0213]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 16 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 4.6541e-04, -4.9017e-04],\n",
      "        [ 1.2981e-04, -1.3671e-04],\n",
      "        [ 8.0008e-05, -8.4264e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 4.6552e-04, -4.9029e-04],\n",
      "        [ 1.2984e-04, -1.3674e-04],\n",
      "        [ 8.0022e-05, -8.4279e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 17 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 0.0035, -0.0027],\n",
      "        [ 0.0072, -0.0055],\n",
      "        [ 0.0321, -0.0243]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 0.0035, -0.0027],\n",
      "        [ 0.0072, -0.0055],\n",
      "        [ 0.0321, -0.0243]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 18 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 3.3175e-04, -3.5443e-04],\n",
      "        [ 7.4838e-05, -7.9955e-05],\n",
      "        [ 3.9811e-05, -4.2534e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 3.3165e-04, -3.5433e-04],\n",
      "        [ 7.4821e-05, -7.9938e-05],\n",
      "        [ 3.9799e-05, -4.2521e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " === Input 19 === \n",
      " =========== COMPARE GRADIENTS =========== \n",
      " --- Our computation:\n",
      " tensor([[ 2.5436e-04, -2.5939e-04],\n",
      "        [ 6.7383e-05, -6.8715e-05],\n",
      "        [ 4.7709e-05, -4.8652e-05]])\n",
      " --- Autograd's computation:\n",
      " tensor([[ 2.5430e-04, -2.5933e-04],\n",
      "        [ 6.7365e-05, -6.8697e-05],\n",
      "        [ 4.7694e-05, -4.8636e-05]])\n",
      "Norm (fc1.weight.grad - model.dL_dw['1']):   0.00\n",
      "Norm (fc2.weight.grad - model.dL_dw['2']):   0.00\n",
      "Norm (fc1.weight.bias - model.dL_db['1']):   0.00\n",
      "Norm (fc2.weight.bias - model.dL_db['2']):   0.00\n",
      "\n",
      " ====  Check that weights have been updated ======\n",
      "Parameter containing:\n",
      "tensor([[0.0714, 0.2209],\n",
      "        [0.2534, 0.4307],\n",
      "        [0.4521, 0.6315]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def compare_with_gradients(model):\n",
    "    print( \" =========== COMPARE GRADIENTS =========== \")\n",
    "    print(\" --- Our computation:\\n\", model.fc1.weight.grad)\n",
    "    print(\" --- Autograd's computation:\\n\",  model.dL_dw['1'])\n",
    "    print(\"Norm (fc1.weight.grad - model.dL_dw['1']):   %.2f\" %torch.norm(model.fc1.weight.grad.flatten() - model.dL_dw['1'].flatten()) )\n",
    "    print(\"Norm (fc2.weight.grad - model.dL_dw['2']):   %.2f\" %torch.norm(model.fc2.weight.grad.flatten() - model.dL_dw['2'].flatten()))\n",
    "    print(\"Norm (fc1.weight.bias - model.dL_db['1']):   %.2f\" %torch.norm(model.fc1.bias.grad.flatten() - model.dL_db['1'].flatten()))\n",
    "    print(\"Norm (fc2.weight.bias - model.dL_db['2']):   %.2f\" %torch.norm(model.fc2.bias.grad.flatten() - model.dL_db['2'].flatten()))\n",
    "    return None\n",
    "\n",
    "def test_backprop(model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(zip(inputs, y_true)):\n",
    "        print(\"\\n === Input %d === \"%i)\n",
    "    \n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        backpropagation(model, y, out)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        compare_with_gradients(model)\n",
    "\n",
    "N = 20\n",
    "inputs = [torch.normal( torch.tensor([-i,i], dtype=torch.float), torch.tensor([1.,2.]) ) for i in range(N)]\n",
    "y_true = [i*torch.ones(2) for i in range(N)]\n",
    "\n",
    "model = MiniNet()\n",
    "model = weight_initialization(model)    \n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "test_backprop(model, optimizer, loss_fn)\n",
    "\n",
    "\n",
    "print(\"\\n ====  Check that weights have been updated ======\")\n",
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing Gradient descent in Pytorch inside the training loop \n",
    "\n",
    "Now that we know how to backpropagate the gradient, the only thing missing to the learning process of our network is how to update parameters using the gradient descent update equation. \n",
    "\n",
    "In the previous section we used a very simple MLP so the only derivatives we had to compute were the derivates of the activation functions. Computing all the terms required by the backpropagation algorithm might become harder when you start including more complex layers such as convolutions. In this section we will go back to using ``loss.backward()`` and will focus on updating parameters once the backpropagation is already done.\n",
    "\n",
    "The update step does not depend on the architecture's complexity. To illustrate this fact, we will now use a LeNet5 model.\n",
    "\n",
    "### 5.0 LetNet5\n",
    "\n",
    "## TODO:\n",
    "\n",
    "Copy paste your favorite implementation of LeNet5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO!\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### 5.1 Manual weight optimization\n",
    "\n",
    "## TODO: (read until the end of this cell)\n",
    "\n",
    "1. Remove the line corresponding to the parameter update step in the training loop below  \n",
    "2. Replace it by a manual update of the parameters using the gradient descent rule $\\theta = \\theta - \\alpha \\nabla L(\\theta)  $   \n",
    "3. Run the '5.2 Compare results` cell to compare with the PyTorch implementation of SGD with consistent optimizer parameters  \n",
    "\n",
    "Once the 3rd point works well (it's normal if values are not exactly equal, but they should follow the same trend):\n",
    "\n",
    "4. Add L2 regularization (using the ``weight_decay`` parameter) to your optimizer.\n",
    "5. Run the '5.3 Compare results` cell (keep the 5.2 one for vanilla SGD so that we can easily compare) to compare with the PyTorch implementation of SGD with consistent optimizer parameters and using regularization\n",
    "\n",
    "Once the 5th point works well:\n",
    "\n",
    "6. Add momentum (using the ``momentum_coeff`` parameter) to your optimizer. **Warning** there are different equations for momentum, use the one used by PyTorch so that comparisons make sense. (See SGD's [note](https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD))\n",
    "7. Run the '5.4 Compare results` cell (keep the 5.3 one for SGD+L2 so that we can easily compare) to compare with the PyTorch implementation of SGD with consistent optimizer parameters and using momentum **WITHOUT** regularization\n",
    "\n",
    "Once the 7th point works well:\n",
    "\n",
    "9. Run the '5.5 Compare results` cell (keep the 5.4 one for SGD+Momentum so that we can easily compare) to compare with the PyTorch implementation of SGD with consistent optimizer parameters and using momentum **WITH** regularization\n",
    "\n",
    "**Hints** \n",
    "\n",
    "- Add the `with torch.no_grad():` before updating parameters\n",
    "- Remember that you can access each of the parameters of your model using for example ``for p in model.parameters()``\n",
    "- Might be useful to detach some tensors at some point :) \n",
    "- Remember to be careful when updating trainable parameters!\n",
    "- You'll still have to re-initialize the gradients after each step (see [torch.Tensor.zero_](https://pytorch.org/docs/stable/tensors.html?highlight=zero_#torch.Tensor.zero_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Device {device}.\")\n",
    "\n",
    "def training_loop_manual_optim(n_epochs, model, loss_fn, train_loader, lr=1e-2, momentum_coeff=0., weight_decay=0.):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # TODO!\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compare results to optim.SGD: vanilla version\n",
    "\n",
    "**NOTE** You can reduce the number of epochs (and also increase how often they are printed) if your computer is slow when you are still trying to make it work. Once it works try to go back to a higher number of epochs before submitting your assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "model = LeNet5()  # TODO: Change this with your own LeNet5 class name\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 31\n",
    "lr = 1e-2\n",
    "\n",
    "print(\"\\n ========= Training using Pytorch's SGD =========\")\n",
    "model01 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "model03 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "training_loop_manual_optim(\n",
    "    n_epochs = n_epochs,\n",
    "    model = model03,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    lr = lr,\n",
    ")\n",
    "\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Training using our SGD =========\")\n",
    "# model02 = copy.deepcopy(model).to(device=device) \n",
    "# # TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "# optimizer = MySGD(model02.parameters(), lr=lr)\n",
    "\n",
    "# training_loop(\n",
    "#     n_epochs = n_epochs,\n",
    "#     optimizer = optimizer,\n",
    "#     model = model02,\n",
    "#     loss_fn = loss_fn,\n",
    "#     train_loader = train_loader,\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n ========= Using Pytorch's SGD =========\")\n",
    "validate(model01, train_loader, val_loader)\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "validate(model03, train_loader, val_loader)\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Using our SGD =========\")\n",
    "# validate(model02, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compare results to optim.SGD: (with Regularization)\n",
    "\n",
    "**NOTE** You can reduce the number of epochs (and also increase how often they are printed) if your computer is slow when you are still trying to make it work. Once it works try to go back to a higher number of epochs before submitting your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "model = LeNet5()  # TODO: Change this with your own LeNet5 class name\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 31\n",
    "lr = 1e-2\n",
    "weight_decay = 0.001\n",
    "\n",
    "print(\"\\n ========= Training using Pytorch's SGD =========\")\n",
    "model01 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "model03 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "training_loop_manual_optim(\n",
    "    n_epochs = n_epochs,\n",
    "    model = model03,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    lr = lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Training using our SGD =========\")\n",
    "# model02 = copy.deepcopy(model).to(device=device) \n",
    "# # TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "# optimizer = MySGD(model02.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# training_loop(\n",
    "#     n_epochs = n_epochs,\n",
    "#     optimizer = optimizer,\n",
    "#     model = model02,\n",
    "#     loss_fn = loss_fn,\n",
    "#     train_loader = train_loader,\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n ========= Using Pytorch's SGD =========\")\n",
    "validate(model01, train_loader, val_loader)\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "validate(model03, train_loader, val_loader)\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Using our SGD =========\")\n",
    "# validate(model02, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compare results to optim.SGD: (with Momentum)\n",
    "\n",
    "**NOTE** You can reduce the number of epochs (and also increase how often they are printed) if your computer is slow when you are still trying to make it work. Once it works try to go back to a higher number of epochs before submitting your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "model = LeNet5()  # TODO: Change this with your own LeNet5 class name\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 31\n",
    "lr = 1e-2\n",
    "momentum_coeff = 0.6\n",
    "\n",
    "\n",
    "print(\"\\n ========= Training using Pytorch's SGD =========\")\n",
    "model01 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr, momentum=momentum_coeff)\n",
    "\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "model03 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "training_loop_manual_optim(\n",
    "    n_epochs = n_epochs,\n",
    "    model = model03,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    lr = lr,\n",
    "    momentum_coeff=momentum_coeff\n",
    ")\n",
    "\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Training using our SGD =========\")\n",
    "# model02 = copy.deepcopy(model).to(device=device) \n",
    "# # TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "# optimizer = MySGD(model02.parameters(), lr=lr, momentum_coeff=momentum_coeff)\n",
    "\n",
    "# training_loop(\n",
    "#     n_epochs = n_epochs,\n",
    "#     optimizer = optimizer,\n",
    "#     model = model02,\n",
    "#     loss_fn = loss_fn,\n",
    "#     train_loader = train_loader,\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n ========= Using Pytorch's SGD =========\")\n",
    "validate(model01, train_loader, val_loader)\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "validate(model03, train_loader, val_loader)\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Using our SGD =========\")\n",
    "# validate(model02, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Compare results to optim.SGD: (with Regularization and Momentum)\n",
    "\n",
    "**NOTE** You can reduce the number of epochs (and also increase how often they are printed) if your computer is slow when you are still trying to make it work. Once it works try to go back to a higher number of epochs before submitting your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=True)\n",
    "model = LeNet5()  # TODO: Change this with your own LeNet5 class name\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 31\n",
    "lr = 1e-2\n",
    "momentum_coeff = 0.6\n",
    "weight_decay = 0.001\n",
    "\n",
    "\n",
    "print(\"\\n ========= Training using Pytorch's SGD =========\")\n",
    "model01 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "optimizer = optim.SGD(model01.parameters(), lr=lr, momentum=momentum_coeff, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer, \n",
    "    model = model01,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "model03 = copy.deepcopy(model).to(device=device) \n",
    "\n",
    "# TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "training_loop_manual_optim(\n",
    "    n_epochs = n_epochs,\n",
    "    model = model03,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    lr = lr,\n",
    "    momentum_coeff=momentum_coeff,\n",
    "    weight_decay=weight_decay,\n",
    "\n",
    ")\n",
    "\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Training using our SGD =========\")\n",
    "# model02 = copy.deepcopy(model).to(device=device) \n",
    "# # TODO: Choose the parameter values consistently so that comparisons make sense\n",
    "# optimizer = MySGD(model02.parameters(), lr=lr, momentum_coeff=momentum_coeff, weight_decay=weight_decay)\n",
    "\n",
    "# training_loop(\n",
    "#     n_epochs = n_epochs,\n",
    "#     optimizer = optimizer,\n",
    "#     model = model02,\n",
    "#     loss_fn = loss_fn,\n",
    "#     train_loader = train_loader,\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"\\n ========= Using Pytorch's SGD =========\")\n",
    "validate(model01, train_loader, val_loader)\n",
    "print(\"\\n ==== Using manual update inside training loop ======\")\n",
    "validate(model03, train_loader, val_loader)\n",
    "# UNCOMMENT ONCE SECTION 6 IS DONE\n",
    "# print(\"\\n ========= Using our SGD =========\")\n",
    "# validate(model02, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementing an Optimizer in PyTorch\n",
    "\n",
    "You might have noticed that when implementing more and more complex optimizers you start polluting a bit too much your training loop. We might prefer to seperate the code consisting in applying the gradient descent method and the code consisting in feeding your model with new inputs. This is what PyTorch does naturally. Indeed, we instantiate an optimizer (e.g. optim.SGD) with all its parameters, (learning rate, momentum, weight_decay, etc) and then we simply need to call ``optimizer.step()``  and ``optimizer.zero_grad()`` in our training loop independently of the optimization method used and of its parameters.\n",
    "\n",
    "Similarly to how we defined our custom neural network in Pytorch, we will create a class to define our custom optimizer. But instead of inheriting the ``nn.Module`` class, we will inherit the [torch.optim.Optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer). As written in the documentation, it is simply the \"Base class for all optimizers\". Keeping the ``nn.Module`` comparison, instead of having to implement a ``forward`` method (regardless of our model) we will have to implement a [step](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step) method (regardless of the optimization method chosen )\n",
    "\n",
    "## TODO:\n",
    "\n",
    "Same questions as above (implement SGD + momentum + L2 successively and compare with PyTorch's implementation by uncommenting the block of code calling ``MySGD``)\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- You might want to add an attribute to your Optimizer when implementing the momentum version\n",
    "- Might be useful to detach some tensors at some point :) \n",
    "- Remember to be careful when updating trainable parameters!\n",
    "- It's okay if you don't succeed here. Try and we will grade generously. :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySGD(Optimizer):\n",
    "    \"\"\"Implements SGD (optionally with momentum and regularization)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        params,\n",
    "        lr=0.01,\n",
    "        momentum_coeff=0.,\n",
    "        weight_decay=0., \n",
    "    ):\n",
    "        # Don't pay attention to this 'default' thing\n",
    "        # it's just something required by PyTorch\n",
    "        defaults = dict(\n",
    "            lr=lr, \n",
    "            momentum_coeff=momentum_coeff,\n",
    "            weight_decay=weight_decay, \n",
    "        )\n",
    "        super().__init__(params, defaults)\n",
    "        self.state = {\"step\": 0}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"  \n",
    "        # Don't pay attention to this 'closure' thing\n",
    "        # it's just something required by PyTorch\n",
    "\n",
    "\n",
    "        # In the optimizer object, the parameters are stored \n",
    "        # in groups for more flexibility (e.g each layer \n",
    "        # could have a different learning rate for example)\n",
    "        # In our case just consider that :\n",
    "        #\n",
    "        # for group in self.param_groups:\n",
    "        #   for p in group['params']:\n",
    "        #\n",
    "        # Is the optimizer counterpart of:\n",
    "        #\n",
    "        # for p in model.parameters()\n",
    "        #\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum_coeff = group['momentum_coeff']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "\n",
    "                    # Now we are safe\n",
    "                    grad = torch.clone(p.grad).detach()\n",
    "      \n",
    "                    # TODO! parameter update\n",
    "\n",
    "        self.state[\"step\"] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
