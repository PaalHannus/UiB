{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I chose to go with \"The Great Gatsby\" to begin with. \n",
    "#https://www.gutenberg.org/ebooks/64317\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "#### Task 1:\n",
    "I choose to go with \"The Great Gatsby\". On second thought it would probably be more entertaining to go with \"Alice In Wonderland\", Dr. Seuce, or some Shakespear\n",
    "\n",
    "#### Task 2:\n",
    "I decided to use some quick regex to remove the unwanted characters, which include \\t, \\n, \\r, \\u200a, \\ufeff. I also removed most whitespace.\n",
    "\n",
    "#### Task 3:\n",
    "I used the Counter object from collections to create a dictionary that has all the characters, and a count of how often it shows up. It is sorted on frequency by default. This dictionary is from character to count. I then created two new dictionaries, the first maps from the character to a number, and the second goes from number to character. Both are needed.\n",
    "\n",
    "#### Task 4:\n",
    "I decided to create a custom pytorch dataset so i could easily utilize the dataloaders. In my first attempt I created the encoding for all the characters in the book when initializing the dataset, it turns out this was a horrible idea. This basically ate up all my ram, and multiple times exceeded the GPU memory. Especially when trying to validate the model. \n",
    "\n",
    "So I changed the dataset-class to compute the onehot-encoding when extracting new data. This works well but slows down training slightly.\n",
    "\n",
    "#### Task 5:\n",
    "I wrote a simple function getDataloaders, that takes the dataset, split_ratio for the train and test sets, and the batch size. Then based on the parameters returns a train and test dataloader.\n",
    "\n",
    "#### Task 6:\n",
    "This in my mind the most tricky part of the assignment. LSTMs and RNNs are strange. Convolutional networks make sense, but LSTMs confuse me... \n",
    "\n",
    "#### Task 7:\n",
    "Quite standard training loop.\n",
    "\n",
    "#### Task 8:\n",
    "I sort of did this step after i did the later steps. But in short, I found a sequence length of 50 characters, 3 lstm layers, hidden size of 256, a batch size of 100 (to cut down on training time), and 0.2 dropout rate. I decided not to use weigth-decay. As the results were quite good as is, and training took a while ;)\n",
    "\n",
    "#### Task 9:\n",
    "As Pierre mentioned in Slack, the model overfit as crazy. So I decided to include dropout regularization. By introducing dropout the model no longer overfit as much. There is still some overfitting, but not as horrible as before.\n",
    "\n",
    "#### Task 10:\n",
    "I wrote a predict function as described. The function takes a model, a seed phrase, and how many characters to predict. \n",
    "\n",
    "#### Task 11:\n",
    "I quickly gets stuck in a loop. Almost instantly\n",
    "\n",
    "#### Task 12:\n",
    "Using torch.distributions i just had to change the original function a little.\n",
    "\n",
    "#### Task 13:\n",
    "(I cropped the outputs, such that they are more \"meaningful\")\n",
    "\n",
    "- Seed: \"The course inf265 is definitely my \"\n",
    "- Result: \"The course inf265 is definitely my bright with the last balance in gold home.\"\n",
    "- Result: \"The course inf265 is definitely my life.\"\n",
    "- Result: \"The course inf265 is definitely my viking her something to flick and now i suppose he wants it had been started at pouch including lightly, where the small business or sneely—when the back and donation something gross is he! had any wild right in a chicago buch of a fair\"\n",
    "\n",
    "\n",
    "- Seed: \"What does the future hold?\"\n",
    "- Result: \"What does the future hold? there’s a little?\"\n",
    "\n",
    "\n",
    "- Seed: \"How many roads must a man walk down \"\n",
    "- Result: \"How many roads must a man walk down the lawn, her watch and mounding in its hair in this noress small last young small, kickies around a family and little colly. “go, myrtle walter to you do not linged all of previvily,” he said, included him, i had an eldered into the flion\"\n",
    "\n",
    "\n",
    "- Seed: \"I am a NLP model and i am \"\n",
    "- Result: \"I am a NLP model and i am my suns\"\n",
    "\n",
    "\n",
    "The model quickly forgets \"where the sentence is going\", so the results are wild to say the least. \n",
    "It was a fun exercise!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BookDataset(Dataset):\n",
    "    def __init__(self, book_path, sequence_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.text = self.preprocessing(book_path)\n",
    "        self.word2int, self.int2word, self.vocab_size = self.__getVocab__(self.text)\n",
    "        self.encoding = self.__generateEncoding__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoding)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoding[idx]\n",
    "    \n",
    "    def __generateEncoding__(self):\n",
    "        txt_as_ints = torch.tensor(list(map(self.word2int.get, self.text)))\n",
    "        subsequences = txt_as_ints.unfold(0, self.sequence_length + 1, 1)\n",
    "        X, y = subsequences[:,:-1], subsequences[:,-1:].squeeze_()\n",
    "        onehot_encoding = F.one_hot(X, self.vocab_size)\n",
    "        return list(zip(onehot_encoding, y))\n",
    "    \n",
    "    def __getVocab__(self, book_txt):\n",
    "        char_count = Counter(book_txt)\n",
    "        int2word = dict(zip(list(range(len(char_count))), sorted(list(char_count.keys()))))\n",
    "        word2int = dict(zip(sorted(list(char_count.keys())), list(range(len(char_count)))))\n",
    "        return word2int, int2word, len(word2int)\n",
    "    \n",
    "    def preprocessing(self, book_path):\n",
    "        pattern = r'[\\t\\n\\r\\u200a\\ufeff]'\n",
    "        book_txt = open(book_path, \"r\", encoding='utf8').read()\n",
    "        book_txt = re.sub(pattern, ' ', book_txt.lower())\n",
    "        book_txt = re.sub('\\s+',' ', book_txt)\n",
    "        return book_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50 #30\n",
    "book_path = './TheGreatGatsby.txt'\n",
    "\n",
    "\n",
    "\n",
    "class BookDatasetLight(Dataset):\n",
    "    def __init__(self, book_path, sequence_length):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.text = self.preprocessing(book_path)\n",
    "        self.word2int, self.int2word, self.vocab_size = self.__getVocab__(self.text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        section = self.text[idx : idx + self.sequence_length + 1]\n",
    "        txt_as_ints = torch.tensor(list(map(self.word2int.get, section)))\n",
    "        X, y = txt_as_ints[:-1], txt_as_ints[-1].unsqueeze_(0)\n",
    "        onehot_encoding = F.one_hot(X, self.vocab_size)\n",
    "\n",
    "        return onehot_encoding, y\n",
    "        \n",
    "    \n",
    "    def __getVocab__(self, book_txt):\n",
    "        char_count = Counter(book_txt)\n",
    "        int2word = dict(zip(list(range(len(char_count))), sorted(list(char_count.keys()))))\n",
    "        word2int = dict(zip(sorted(list(char_count.keys())), list(range(len(char_count)))))\n",
    "        return word2int, int2word, len(word2int)\n",
    "    \n",
    "    def preprocessing(self, book_path):\n",
    "        pattern = r'[\\t\\n\\r\\u200a\\ufeff]'\n",
    "        book_txt = open(book_path, \"r\", encoding='utf8').read()\n",
    "        book_txt = re.sub(pattern, ' ', book_txt.lower())\n",
    "        book_txt = re.sub('\\s+',' ', book_txt)\n",
    "        return book_txt\n",
    "\n",
    "def getDataloaders(dataset, split_ratio, batch_size):\n",
    "    length = dataset.__len__()\n",
    "    test_length = int(length * split_ratio)\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [length - test_length, test_length])\n",
    "    train_generator = DataLoader(train_dataset, batch_size = batch_size)\n",
    "    test_generator = DataLoader(test_dataset, batch_size = batch_size)\n",
    "    return train_generator, test_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.2\n",
    "batch_size = 100\n",
    "dataset = BookDatasetLight(book_path, sequence_length)\n",
    "train_generator, test_generator = getDataloaders(dataset, split_ratio, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dataset.vocab_size\n",
    "vocab_size = dataset.vocab_size\n",
    "hidden_size = 256\n",
    "n_layers = 3\n",
    "\n",
    "\n",
    "\n",
    "class TxtNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, batch_size, sequence_length, batch_first = True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, n_layers, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm_layer(x, (h0, c0))#, self.init_state(self.sequence_length))\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = TxtNet(input_size = input_size, hidden_size = hidden_size, n_layers = n_layers, batch_size = batch_size, sequence_length = 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def training_model(n_epochs, optimizer, model, train_loader, validation_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(f\"Epoch: {epoch}, begins!\")\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        for batch, (x,y) in enumerate(train_loader):\n",
    "\n",
    "            x = x.type(torch.FloatTensor).to(device)\n",
    "            y.squeeze_()\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            \n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f\"Mean training loss in epoch {epoch}: {round(train_loss / len(train_loader), 3)}\")\n",
    "        \n",
    "        validation_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                x = x.type(torch.FloatTensor).to(device)\n",
    "                y.squeeze_()\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "            print(f\"Mean validation loss: {round(validation_loss / len(validation_loader), 3)}\")\n",
    "\n",
    "def validate_model(model, validation_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    validation_loss = 0.0\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.type(torch.FloatTensor).to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Mean validation loss: {round(validation_loss / len(validation_loader), 3)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, begins!\n",
      "Mean training loss in epoch 1: 2.447\n",
      "Mean validation loss: 2.002\n",
      "Epoch: 2, begins!\n",
      "Mean training loss in epoch 2: 1.85\n",
      "Mean validation loss: 1.702\n",
      "Epoch: 3, begins!\n",
      "Mean training loss in epoch 3: 1.639\n",
      "Mean validation loss: 1.569\n",
      "Epoch: 4, begins!\n",
      "Mean training loss in epoch 4: 1.522\n",
      "Mean validation loss: 1.495\n",
      "Epoch: 5, begins!\n",
      "Mean training loss in epoch 5: 1.444\n",
      "Mean validation loss: 1.447\n",
      "Epoch: 6, begins!\n",
      "Mean training loss in epoch 6: 1.383\n",
      "Mean validation loss: 1.416\n",
      "Epoch: 7, begins!\n",
      "Mean training loss in epoch 7: 1.337\n",
      "Mean validation loss: 1.395\n",
      "Epoch: 8, begins!\n",
      "Mean training loss in epoch 8: 1.299\n",
      "Mean validation loss: 1.384\n",
      "Epoch: 9, begins!\n",
      "Mean training loss in epoch 9: 1.264\n",
      "Mean validation loss: 1.374\n",
      "Epoch: 10, begins!\n",
      "Mean training loss in epoch 10: 1.235\n",
      "Mean validation loss: 1.369\n",
      "Epoch: 11, begins!\n",
      "Mean training loss in epoch 11: 1.208\n",
      "Mean validation loss: 1.365\n",
      "Epoch: 12, begins!\n",
      "Mean training loss in epoch 12: 1.184\n",
      "Mean validation loss: 1.367\n",
      "Epoch: 13, begins!\n",
      "Mean training loss in epoch 13: 1.163\n",
      "Mean validation loss: 1.367\n",
      "Epoch: 14, begins!\n",
      "Mean training loss in epoch 14: 1.142\n",
      "Mean validation loss: 1.371\n",
      "Epoch: 15, begins!\n",
      "Mean training loss in epoch 15: 1.124\n",
      "Mean validation loss: 1.376\n"
     ]
    }
   ],
   "source": [
    "training_model(15, optimizer, model, train_generator, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The course inf265 is definitely my hand and i saw that the country of the couch, and i saw that the couch, and i saw that the country a\n",
      "\n",
      "I am a NLP model and i am my suns, jeyeing them with a high like to the phone, young played and lequin and that abraid the orchestra room. “i’ve gats—this abroak of whom on you’re going.” “they’re some many-singing breath and conteqperal movements with asping myster, and i lo\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "w2int = deepcopy(dataset.word2int)\n",
    "int2w = deepcopy(dataset.int2word)\n",
    "\n",
    "def predict(model, seed, steps):\n",
    "    model.eval()\n",
    "    txt_as_ints = torch.tensor(list(map(w2int.get, seed.lower())))\n",
    "    output = seed\n",
    "    onehot_encoding = F.one_hot(txt_as_ints, len(w2int))\n",
    "    onehot_encoding = onehot_encoding.type(torch.FloatTensor)\n",
    "    onehot_encoding = onehot_encoding.unsqueeze_(0).to(device)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        out = model(onehot_encoding.to(device))\n",
    "        predicted_char = torch.argmax(F.softmax(out, 1))\n",
    "        output = output + int2w[predicted_char.item()]\n",
    "        \n",
    "        prediction_onehot = F.one_hot(torch.tensor(predicted_char.item()), len(w2int))\n",
    "        \n",
    "        temp_tensor = torch.zeros(onehot_encoding.shape)\n",
    "        temp_tensor[:,0:-1,:] = onehot_encoding[:,1:,:]\n",
    "        temp_tensor[:,-1,:] = prediction_onehot\n",
    "        onehot_encoding = temp_tensor\n",
    "        \n",
    "    \n",
    "    return output\n",
    "\n",
    "def predict_probability(model, seed, steps):\n",
    "    model.eval()\n",
    "    txt_as_ints = torch.tensor(list(map(w2int.get, seed.lower())))\n",
    "    output = seed\n",
    "    onehot_encoding = F.one_hot(txt_as_ints, len(w2int))\n",
    "    onehot_encoding = onehot_encoding.type(torch.FloatTensor)\n",
    "    onehot_encoding = onehot_encoding.unsqueeze_(0).to(device)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        out = model(onehot_encoding.to(device))\n",
    "\n",
    "        dist = Categorical(F.softmax(out, 1))\n",
    "        index = dist.sample().item()\n",
    "        \n",
    "        output = output + int2w[index]\n",
    "        \n",
    "        prediction_onehot = F.one_hot(torch.tensor(index), len(w2int))\n",
    "        \n",
    "        temp_tensor = torch.zeros(onehot_encoding.shape)\n",
    "        temp_tensor[:,0:-1,:] = onehot_encoding[:,1:,:]\n",
    "        temp_tensor[:,-1,:] = prediction_onehot\n",
    "        onehot_encoding = temp_tensor\n",
    "        \n",
    "    \n",
    "    return output\n",
    "\n",
    "print(predict(model, \"The course inf265 is definitely my \", 100))\n",
    "print()\n",
    "print(predict_probability(model, \"I am a NLP model and i am \", 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model_seq50_gatsby.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load('model_seq50_gatsby.pt.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, 'a': 29, 'b': 30, 'c': 31, 'd': 32, 'e': 33, 'f': 34, 'g': 35, 'h': 36, 'i': 37, 'j': 38, 'k': 39, 'l': 40, 'm': 41, 'n': 42, 'o': 43, 'p': 44, 'q': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'x': 52, 'y': 53, 'z': 54, 'ç': 55, 'é': 56, 'ê': 57, 'ô': 58, '—': 59, '‘': 60, '’': 61, '“': 62, '”': 63, '…': 64}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "#  IGNORE THIS, SOME LEFTOVER CODE\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessbook(book_txt):\n",
    "    ''' Simple regex to remove unwanted chars and ensure A single space between chars'''\n",
    "    pattern = r'[\\t\\n\\r\\u200a\\ufeff]'\n",
    "    book_txt = re.sub(pattern, ' ', book_txt.lower())\n",
    "    book_txt = re.sub('\\s+',' ', book_txt)\n",
    "    return book_txt\n",
    "\n",
    "book_path = './TheGreatGatsby.txt'\n",
    "\n",
    "txt = open(book_path, \"r\", encoding='utf8').read()\n",
    "book = preprocessbook(txt)\n",
    "\n",
    "char_count = Counter(book)\n",
    "vocab = dict(zip(list(range(len(char_count))), sorted(list(char_count.keys()))))\n",
    "vocab2int = dict(zip(sorted(list(char_count.keys())), list(range(len(char_count)))))\n",
    "\n",
    "print(vocab2int)\n",
    "\n",
    "\n",
    "def fromBookToData(txt, sequence_length = sequence_length):\n",
    "    txt_as_ints = torch.tensor(list(map(vocab2int.get, txt)))\n",
    "    subsequences = txt_as_ints.unfold(0, sequence_length + 1, 1)\n",
    "    X, y = subsequences[:,:-1], subsequences[:,-1:].squeeze_()\n",
    "    onehot_encoding = F.one_hot(X, len(vocab))\n",
    "    return onehot_encoding, y\n",
    "\n",
    "def partitionData(data, test_size = 0.2):\n",
    "    data_pairs = list(zip(data[0], data[1])) #[:10000]\n",
    "    data_len = len(data_pairs)\n",
    "    test_len = int(len(data_pairs) * test_size)\n",
    "    train, val = random_split(data_pairs, [data_len - test_len, test_len])\n",
    "    return train, val\n",
    "\n",
    "\n",
    "b = book[:10000]\n",
    "train_data, val_data = partitionData(fromBookToData(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_t = torch.rand((1,3,6))\n",
    "print(test_t)\n",
    "\n",
    "print(test_t[:,:,1:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
