{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN():\n",
    "    def __init__(self, layers):\n",
    "        self._layers = layers\n",
    "        self._params = {}\n",
    "        self.grad = {}\n",
    "        self.activation = {\n",
    "            \"relu\" : self.relu,\n",
    "            \"sigmoid\" : self.sigmoid,\n",
    "        }\n",
    "        self.derivations = {\n",
    "            \"relu\" : self.d_relu,\n",
    "            \"sigmoid\" : self.d_sigmoid\n",
    "        }\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        self._params['W'] = {}\n",
    "        self._params['B'] = {}\n",
    "        for layer in (range(1, len(self._layers))):\n",
    "            \n",
    "            self._params['W'][layer] = np.random.rand(self._layers[layer], self._layers[layer-1])\n",
    "            self._params['B'][layer] = np.zeros((self._layers[layer], 1)) #np.random.rand(self._layers[layer], 1)\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1.0/(1 + np.exp(-val))\n",
    "    \n",
    "    def d_sigmoid(self, val):\n",
    "        return np.exp(-val) / (np.exp(-val)+1)**2\n",
    "    \n",
    "    def relu(self, val):\n",
    "        return np.where(val > 0, val, 0)\n",
    "\n",
    "    def d_relu(self, val):\n",
    "        return np.where(val > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, val):\n",
    "        exps = np.exp(val - val.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "    def d_softmax(self, val):\n",
    "        exps = np.exp(val - val.max())\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "    \n",
    "    def cross_entropy(self, predictions, targets, epsilon=1e-9):\n",
    "        predictions = np.clip(predictions.squeeze(), epsilon, 1. - epsilon)\n",
    "        \n",
    "        N = predictions.shape[0]\n",
    "        ce = -np.sum(targets*np.log(predictions))/N\n",
    "        return ce\n",
    "    \n",
    "    def predict(self, X):\n",
    "        A = X.T\n",
    "        \n",
    "        for layer in range(1, len(self._layers)-1):\n",
    "            Z = np.dot(self._params['W'][layer], A) + self._params['B'][layer]\n",
    "            A = self.sigmoid(Z)\n",
    "        Z = np.dot(self._params['W'][len(self._layers)-1], A) + self._params['B'][len(self._layers)-1]\n",
    "        A = self.softmax(Z)\n",
    "        return A\n",
    "    \n",
    "    def forward(self, X):\n",
    "        grad = {\n",
    "            'A' : {},\n",
    "            'Z' : {},\n",
    "        }\n",
    "        A = X.T\n",
    "        grad['A'][0] = A\n",
    "        \n",
    "        for layer in range(1, len(self._layers)-1):\n",
    "            Z = np.dot(self._params['W'][layer], A) + self._params['B'][layer]\n",
    "            A = self.sigmoid(Z)\n",
    "            grad['A'][layer] = A\n",
    "            grad['Z'][layer] = Z\n",
    "        \n",
    "        Z = np.dot(self._params['W'][len(self._layers)-1], A) + self._params['B'][len(self._layers)-1]\n",
    "        A = self.softmax(Z)\n",
    "        \n",
    "        grad[\"A\"][len(self._layers) - 1] = A\n",
    "        grad[\"Z\"][len(self._layers) - 1] = Z\n",
    "        self.grad = grad\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backwards(self, y_true):\n",
    "        n_layers = len(self._layers)\n",
    "        W = self._params['W']\n",
    "        B = self._params['B']\n",
    "        grad = self.grad\n",
    "        \n",
    "        gradients = {\n",
    "            'Z' : {},\n",
    "            'W' : {},\n",
    "            'b' : {}\n",
    "        }\n",
    "        for layer in reversed(range(1,n_layers)):\n",
    "            if layer == n_layers-1:\n",
    "                dZ = grad['A'][n_layers-1].reshape(grad['Z'][n_layers-1].shape) - y_true.reshape(grad['Z'][n_layers-1].shape)\n",
    "            else:\n",
    "                dZ = np.dot(W[layer+1].T, dZ) * self.d_sigmoid(grad['Z'][layer])\n",
    "            \n",
    "            dW = np.dot(dZ, grad['A'][layer-1].T)\n",
    "            db = dZ\n",
    "            \n",
    "            gradients['Z'][layer] = dZ\n",
    "            gradients['W'][layer] = dW\n",
    "            gradients['b'][layer] = db\n",
    "        self.gradients = gradients\n",
    "        return gradients\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        for layer in range(1, len(self._layers)):\n",
    "            before = self._params['W'][layer].copy()\n",
    "            self._params['W'][layer] -= lr * self.gradients['W'][layer]\n",
    "            self._params['B'][layer] -= lr * self.gradients['b'][layer]\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        self.gradients = {}\n",
    "        \n",
    "    def train(self, data, labels, n_epochs, lr=0.01):\n",
    "        error = []\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            for x, y in zip(data,labels):\n",
    "                \n",
    "                pred = self.forward(x)\n",
    "                epoch_loss = epoch_loss + self.cross_entropy(pred, y)\n",
    "                self.backwards(y)\n",
    "                self.optimize(lr)\n",
    "                \n",
    "            error.append(epoch_loss / len(labels))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleNet = FeedforwardNN([2,9,2])\n",
    "\n",
    "# XOR function\n",
    "data = [np.array([[0,0]]), np.array([[0,1]]), np.array([[1,0]]), np.array([[1,1]])]\n",
    "labels = [np.array([[0,1]]), np.array([[1,0]]), np.array([[1,0]]), np.array([[0,1]])]\n",
    "\n",
    "error = simpleNet.train(data, labels, 10000, lr=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00124636 0.99875364]\n",
      "0.0006235699783381693\n"
     ]
    }
   ],
   "source": [
    "pred = simpleNet.predict(np.array([[1,1]])).reshape(2,)\n",
    "print(pred)\n",
    "loss = simpleNet.cross_entropy(pred, np.array([[0,1]]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeData(data):\n",
    "    res = np.zeros((data.size, data.max()+1))\n",
    "    res[np.arange(data.size),data] = 1\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = list(iris.data)\n",
    "X = [np.array([x]) for x in X]\n",
    "y = list(encodeData(iris.target))\n",
    "y = [np.array([i]) for i in y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = FeedforwardNN([4,5,3])\n",
    "nn.train(X,y, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98733027e-01]\n",
      " [8.71122760e-04]\n",
      " [3.95850000e-04]]\n",
      "[[1. 0. 0.]]\n",
      "\n",
      "[[9.98618231e-01]\n",
      " [9.57873367e-04]\n",
      " [4.23895378e-04]]\n",
      "[[1. 0. 0.]]\n",
      "\n",
      "[[0.00118682]\n",
      " [0.98815061]\n",
      " [0.01066257]]\n",
      "[[0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(nn.predict(X[0]))\n",
    "print(y[0])\n",
    "print()\n",
    "print(nn.predict(X[20]))\n",
    "print(y[20])\n",
    "print()\n",
    "print(nn.predict(X[50]))\n",
    "print(y[50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN():\n",
    "    def __init__(self, layers):\n",
    "        self._layers = layers\n",
    "        self._params = {}\n",
    "        self.grad = {}\n",
    "        self.activation = {\n",
    "            \"relu\" : self.relu,\n",
    "            \"sigmoid\" : self.sigmoid,\n",
    "        }\n",
    "        self.derivations = {\n",
    "            \"relu\" : self.d_relu,\n",
    "            \"sigmoid\" : self.d_sigmoid\n",
    "        }\n",
    "        self.__init_weights()\n",
    "\n",
    "    def __init_weights(self):\n",
    "        self._params['W'] = {}\n",
    "        self._params['B'] = {}\n",
    "        for layer in (range(1, len(self._layers))):\n",
    "            \n",
    "            self._params['W'][layer] = np.random.rand(self._layers[layer], self._layers[layer-1])\n",
    "            self._params['B'][layer] = np.zeros((self._layers[layer], 1)) #np.random.rand(self._layers[layer], 1)\n",
    "\n",
    "    def sigmoid(self, val):\n",
    "        return 1.0/(1 + np.exp(-val))\n",
    "    \n",
    "    def d_sigmoid(self, val):\n",
    "        return np.exp(-val) / (np.exp(-val)+1)**2\n",
    "    \n",
    "    def relu(self, val):\n",
    "        return np.where(val > 0, val, 0)\n",
    "\n",
    "    def d_relu(self, val):\n",
    "        return np.where(val > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, val):\n",
    "        exps = np.exp(val - val.max())\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "    \n",
    "    def d_softmax(self, val):\n",
    "        exps = np.exp(val - val.max())\n",
    "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "    \n",
    "    def cross_entropy_loss(self, Y_pred, Y_true): # Ta mean av loss function for Ã¥ finne cost (Andrew Ng)\n",
    "        Y_pred = Y_pred.squeeze()\n",
    "        Y_true = Y_true.squeeze()\n",
    "        res = - (Y_true*(np.log(Y_pred)) + (1-Y_true)*np.log(1-Y_pred))\n",
    "        res = np.expand_dims(res, axis=0)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        grad = {\n",
    "            'A' : {},\n",
    "            'Z' : {},\n",
    "        }\n",
    "        A = np.expand_dims(X, axis=0).T\n",
    "        grad['A'][0] = A\n",
    "        \n",
    "        for layer in range(1, len(self._layers)-1):\n",
    "            Z = np.dot(self._params['W'][layer], A) + self._params['B'][layer]\n",
    "            A = self.sigmoid(Z)\n",
    "            grad['A'][layer] = A\n",
    "            grad['Z'][layer] = Z\n",
    "        \n",
    "        Z = np.dot(self._params['W'][len(self._layers)-1], A) + self._params['B'][len(self._layers)-1]\n",
    "        A = self.softmax(Z)\n",
    "        \n",
    "        grad[\"A\"][len(self._layers) - 1] = A\n",
    "        grad[\"Z\"][len(self._layers) - 1] = Z\n",
    "        self.grad = grad\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backpropagation(self, y_true, y_pred):\n",
    "        n_layers = len(self._layers)\n",
    "        W = self._params['W']\n",
    "        B = self._params['B']\n",
    "        grad = self.grad\n",
    "        \n",
    "        gradients = {\n",
    "            'Z' : {},\n",
    "            'W' : {},\n",
    "            'b' : {}\n",
    "        }\n",
    "        \n",
    "        for layer in reversed(range(1,n_layers)):\n",
    "            if layer == n_layers-1:\n",
    "                dZ = grad['A'][n_layers-1].reshape(grad['Z'][n_layers-1].shape) - y_true.reshape(grad['Z'][n_layers-1].shape)\n",
    "            else:\n",
    "                dZ = np.dot(W[layer+1].T, dZ) * self.d_sigmoid(grad['Z'][layer])\n",
    "            \n",
    "            dW = np.dot(dZ, grad['A'][layer-1].T)\n",
    "            db = dZ\n",
    "            \n",
    "            gradients['Z'][layer] = dZ\n",
    "            gradients['W'][layer] = dW\n",
    "            gradients['b'][layer] = db\n",
    "            \n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def backpropagation_testing(self, y_true, y_pred):\n",
    "        \n",
    "        n_layers = len(self._layers)\n",
    "        W = self._params['W']\n",
    "        B = self._params['B']\n",
    "        grad = self.grad\n",
    "        \n",
    "        print(\"W\")\n",
    "        [print(\"  \", i, x.shape) for i,x in W.items()]\n",
    "        print(\"A\")\n",
    "        [print(\"  \", i, x.shape) for i,x in grad['A'].items()]\n",
    "        print(\"Z\")\n",
    "        [print(\"  \", i, x.shape) for i,x in grad['Z'].items()]\n",
    "        \n",
    "        print()\n",
    "        print(\"-----Testing------\")\n",
    "        \n",
    "        dZ2 = grad['A'][2].reshape(grad['Z'][2].shape) - y_true.reshape(grad['Z'][2].shape)\n",
    "        print(dZ2.shape)\n",
    "        \n",
    "        dW2 = np.dot(dZ2, grad['A'][1].T)\n",
    "        print(dW2.shape)\n",
    "        \n",
    "        dB2 = dZ2\n",
    "        print(dB2.shape)\n",
    "        \n",
    "        dZ1 = np.dot(W[2].T, dZ2) * self.d_sigmoid(grad['Z'][1])\n",
    "        print(dZ1.shape)\n",
    "        \n",
    "        dW1 = np.dot(dZ1, grad['A'][0].T)\n",
    "        print(dW1.shape)\n",
    "        \n",
    "        dB1 = dZ1\n",
    "        print(dB1.shape)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        gradients = {\n",
    "            'Z' : {},\n",
    "            'W' : {},\n",
    "            'b' : {}\n",
    "        }\n",
    "        \n",
    "        for layer in reversed(range(1,n_layers)):\n",
    "            if layer == n_layers-1:\n",
    "                dZ = grad['A'][2].reshape(grad['Z'][n_layers-1].shape) - y_true.reshape(grad['Z'][n_layers-1].shape)\n",
    "            else:\n",
    "                dZ = np.dot(W[layer+1].T, dZ2) * self.d_sigmoid(grad['Z'][layer])\n",
    "            \n",
    "            dW = np.dot(dZ, grad['A'][layer-1].T)\n",
    "            db = dZ\n",
    "            \n",
    "            gradients['Z'][layer] = dZ\n",
    "            gradients['W'][layer] = dW\n",
    "            gradients['b'][layer] = db\n",
    "            \n",
    "        return gradients\n",
    "        \n",
    "# w[2] = (n[2], n[1]) dimensional\n",
    "# Z[2],dZ[2] = (n[2], 1)\n",
    "# Z[1],dZ[1] = (n[1], 1)\n",
    "\n",
    "#W.shape == dW.shape\n",
    "#Z.shape == dZ.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z': {3: array([[-0.32877173],\n",
       "         [-1.67122827]]),\n",
       "  2: array([[-0.24394369],\n",
       "         [-0.31304895],\n",
       "         [-0.06790937],\n",
       "         [-0.09004923]]),\n",
       "  1: array([[-0.0226095 ],\n",
       "         [-0.02705689],\n",
       "         [-0.04391744],\n",
       "         [-0.0364433 ]])},\n",
       " 'W': {3: array([[-0.22936223, -0.25064715, -0.25726896, -0.25982073],\n",
       "         [-1.1659051 , -1.27410164, -1.30776197, -1.32073324]]),\n",
       "  2: array([[-0.2030698 , -0.21530939, -0.19333892, -0.19907659],\n",
       "         [-0.26059615, -0.27630302, -0.24810867, -0.25547174],\n",
       "         [-0.05653084, -0.05993811, -0.05382194, -0.05541921],\n",
       "         [-0.07496107, -0.07947918, -0.07136902, -0.07348702]]),\n",
       "  1: array([[-0.0226095 , -0.0226095 , -0.0226095 ],\n",
       "         [-0.02705689, -0.02705689, -0.02705689],\n",
       "         [-0.04391744, -0.04391744, -0.04391744],\n",
       "         [-0.0364433 , -0.0364433 , -0.0364433 ]])},\n",
       " 'b': {3: array([[-0.32877173],\n",
       "         [-1.67122827]]),\n",
       "  2: array([[-0.24394369],\n",
       "         [-0.31304895],\n",
       "         [-0.06790937],\n",
       "         [-0.09004923]]),\n",
       "  1: array([[-0.0226095 ],\n",
       "         [-0.02705689],\n",
       "         [-0.04391744],\n",
       "         [-0.0364433 ]])}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleNet = FeedforwardNN([3,4, 4,2])\n",
    "#simpleNet._params['W'][1] = np.array([1.0,1.0,1.0,1.0,1.0,1.0]).reshape(3,2)\n",
    "#simpleNet._params['W'][2] = np.array([1.0,1.0,1.0,1.0,1.0,1.0]).reshape(2,3)\n",
    "\n",
    "\n",
    "out = simpleNet.forward_prop(np.array([1,1,1]))\n",
    "#print(out)\n",
    "\n",
    "#print(simpleNet.cross_entropy_loss(out, np.array([1,])))\n",
    "\n",
    "simpleNet.backpropagation(np.array([1, 2]).squeeze(), out.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(1,3)\n",
    "b = np.random.rand(3)\n",
    "\n",
    "print(b.shape)\n",
    "b = np.expand_dims(b, axis=0)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #error = {}\n",
    "        #delta = {}\n",
    "       # \n",
    "        #error[n_layers-1] = np.expand_dims(y_true.squeeze()-y_pred.squeeze(), axis=0)\n",
    "        #delta[n_layers] = np.multiply(error[n_layers-1], self.d_sigmoid(y_pred))\n",
    "       # \n",
    "        #for layer in reversed(range(1, len(self._layers))):\n",
    "        ##    print(layer)\n",
    "        #    print(W[layer].shape, delta[layer+1].shape)\n",
    "        #    print(np.dot(delta[layer+1], W[layer]).shape, self.d_sigmoid(grad['A'][layer]).shape)\n",
    "        #    delta[layer] = np.multiply(np.dot(delta[layer+1], W[layer]), self.d_sigmoid(grad['A'][layer]))\n",
    "            \n",
    "            \n",
    "            #error[layer] = np.dot(delta[layer+1], W[layer])\n",
    "            #delta[layer] = np.multiply(error[layer], self.d_sigmoid(grad['A'][layer]))\n",
    "            \n",
    "            \n",
    "            #error[layer] = np.dot( error[layer + 1], np.dot(self._params['W'][layer], self.derivations['sigmoid'](self.grad['A'][layer])))\n",
    "            #print(delta[layer].shape)\n",
    "            \n",
    "            #e_prev_layer = np.expand_dims(error[layer + 1], axis=0)\n",
    "            \n",
    "            #error[layer] = e_prev_layer * self._params['W'][layer] #* self.derivations['sigmoid'](self.grad['A'][layer])\n",
    "        \n",
    "        #error = {}\n",
    "        #delta = {}\n",
    "        \n",
    "        #Output layer\n",
    "        #error[len(self._layers)-1] = y_true - y_pred\n",
    "        #delta[len(self._layers)-1] = error[len(self._layers)-1] * self.derivations['sigmoid'](y_pred)\n",
    "        \n",
    "        #for layer in reversed(range(len(self._layers)-1)):\n",
    "        #    error[layer] = np.dot(self._params['W'][layer+1].T, error[layer+1])\n",
    "        #    delta[layer] = error[layer+1] * self.derivations['sigmoid'](self.grad['A'][layer])\n",
    "        #    \n",
    "        #    print(\"layer\", layer)\n",
    "        #    \n",
    "        #    print(delta[layer].shape)\n",
    "        \n",
    "        \n",
    "            print(\"-----Testing------\")\n",
    "        \n",
    "        print(n_layers)\n",
    "        \n",
    "        dZ2 = grad['A'][2].reshape(grad['Z'][2].shape) - y_true.reshape(grad['Z'][2].shape)\n",
    "        print(dZ2)\n",
    "        \n",
    "        dW2 = np.dot(dZ2, grad['A'][1].T)\n",
    "        print(dW2.shape)\n",
    "        \n",
    "        dB2 = dZ2\n",
    "        print(dB2.shape)\n",
    "        \n",
    "        dZ1 = np.dot(W[2].T, dZ2) * self.d_sigmoid(grad['Z'][1])\n",
    "        print(dZ1.shape)\n",
    "        \n",
    "        dW1 = np.dot(dZ1, grad['A'][0].T)\n",
    "        print(dW1.shape)\n",
    "        \n",
    "        dB1 = dZ1\n",
    "        print(dB1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
