{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1 - *Linguistic Essentials and Collocations*\n",
    "\n",
    "**NOTE**: I decided to write the entire report in this notebook. Everything you will need to run the program is found here. \n",
    "\n",
    "### Problem introduction:\n",
    "\n",
    "A collocation is a phrase consisting of more than one word but these words more commonly co-occur in a given context than its individual word parts. For example, in a set of hospital related documents, the phrase ‘CT scan’ is more likely to co-occur than do ‘CT’ and ‘scan’ individually. ‘CT scan’ is also a meaningful phrase. Additionally they simply sound \"right\" to a native speaker. To capture the essence of a language it is important that NLP-models capture and include collocations. \n",
    "\n",
    "The task of identifying collocations felt quite daunting at first. But using large corpora it is really not that challenging. There are two methods tried and tested in this notebook. Firstly identifying collocations using their frequency. This is the simplest way of detecting collocations. It works by simply counting the frequency of each bigram; Bigrams that occur frequently are likely to be collocations. Hence referred to as the frequency method. \n",
    "The second method in this notebook is the hypothesis test method (t-test). It is a simple statistical test where we assume that the two words are independent of each other. By comparing how often the words occur in total and how often they occur in conjuction we can use it as statistical evidence for them being a collocation or not. *If a word occurs much more frequently next to another spesific word than it does by alone, it is likely a collocation*.\n",
    "\n",
    "\n",
    "![https://miro.medium.com/max/1205/1*-jIXJtKo0cq9UBY-WDX9Aw.png](hyp_1.png)\n",
    "\n",
    "\n",
    "![https://miro.medium.com/max/803/1*NR6uUN5N9IJCqOCTUomE9g.png](hyp_2.png)\n",
    "\n",
    "\n",
    "Being a collocation, \"social media\" (as seen in the example above) should be much more frequent than if the two words were truly independent.\n",
    "\n",
    "\n",
    "### Installation:\n",
    "\n",
    "Firstly the notebook requires both - jupyter notebooks and python. Once both are installed, we simply need to install the natural language toolkit NLTK with a package manager. Pip being the simplest. (**pip install nltk**). No other downloads are required as the csv package is included in standard python.\n",
    "\n",
    "**NOTE**: When installing the toolkit, be sure to include the corpora.\n",
    "\n",
    "### My approach: \n",
    "\n",
    "I actually did this in two ways originally (although I only included the methods below). First I solved 1.1 by using the functions/objects from the toolkit. *BigramCollocationFinder* from nltk.collocations being the most important. \n",
    "Then I pivoted, and chose to implement the functions \"from scratch\"... sort of. I used the already tagged words from the corpus to filter the bigrams, and used nltk.ngrams to generate all the bigrams. I decided to only include bigram-collocations where the first word is either a noun or an adjective, and the second word is a noun. Some collocations have a different structure than this, and it can easily be changed in the code. It was mostly for simplicity. \n",
    "Anyway, I decided to do it this way to make sure i understood the assignment correctly. And since i got the same results using both the builtin-approach and the custom-approach. I decided only to include one of them. Here are some articles explaining the builtin-approach (I found the medium article especially useful).\n",
    "\n",
    "https://medium.com/@nicharuch/collocations-identifying-phrases-that-act-like-individual-words-in-nlp-f58a93a2f84a\n",
    "\n",
    "http://www.nltk.org/howto/collocations.html\n",
    "\n",
    "https://towardsdatascience.com/collocations-in-nlp-using-nltk-library-2541002998db\n",
    "\n",
    "Both functions (the frequency and hypothesis methods) include a parameter to change the corpus, and the threshold. The threshold is a way to filter out words that show little evidence for being collocations. \n",
    "\n",
    "When running the notebook two files will generate, \"hyp.csv\" and \"freq.csv\". Each of which contains the collocations found by each respective method. To correct the non-natural expressions the function fixCollocations is used. It takes a variable number of arguments, all of which will be used to generate bigrams. It is also possible to change the filename to use collocations stored in a different file. The function loops over all adjecent pairs of input arguments and checks if they contain any synonyms with the collocations stored on the datafile. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function design choice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frequencyMethod(threshold = 3, corpus = brown):\n",
    "    ''' \n",
    "    A function to generate \"estimated\" collocations using the frequency method as described in lecture 2. \n",
    "    Additionally, the first word in the can be either nouns or adjectives, the second word must be a noun.\n",
    "    \n",
    "    Params:\n",
    "    - threshold: Set a lower bounds for frequencies. Defaults to 3.\n",
    "    - corpus: The text corpus used for the calculations. Defaults to the \"brown\" corpus.\n",
    "    \n",
    "    Output:\n",
    "    - A list containing bigrams that occur frequently. Including the frequency.\n",
    "    '''\n",
    "    \n",
    "    freq = {}\n",
    "    tags = corpus.tagged_words(tagset='universal')\n",
    "    bigram_tags = list(nltk.ngrams(tags, 2))\n",
    "    bigrams_filtered = list(filter(lambda x:x[0][1] in ['NOUN', 'ADJ'] and x[1][1] == 'NOUN', bigram_tags))\n",
    "    \n",
    "    for ((w1,t1),(w2,t2)) in bigrams_filtered:\n",
    "        curr = freq.get(w1+\" \"+w2,0) # Combine words - use combination as a key\n",
    "        freq[w1+\" \"+w2] = curr + 1\n",
    "    \n",
    "    freq_filtered = {k: v for k,v in freq.items() if v > threshold}\n",
    "    return freq_filtered\n",
    "\n",
    "\n",
    "\n",
    "def hypothesisMethod(threshold = 2.576, corpus = brown):\n",
    "    '''\n",
    "    A function to generate \"estimated\" collocations using the hypothesis (t-test) method as described in lecture 2.\n",
    "    Additionally, the first word in the can be either nouns or adjectives, the second word must be a noun.\n",
    "    \n",
    "    First loops over words, and bigrams to get a count of each. Then loops over unique bigrams in the corpus and\n",
    "    calculates the t-value of each. The formula can be found on the slides of lecture 2, or below in the report.\n",
    "    \n",
    "    Params:\n",
    "    - threshold: Set a lower bounds for confidence. Defaults to 2.576 ~ 99.5% confidence.\n",
    "    - corpus: The text corpus used for the calculations. Defaults to the \"brown\" corpus.\n",
    "    \n",
    "    Output:\n",
    "    - A list containing bigrams that score the highest / have the highest t value. Also includes the t-value.\n",
    "    '''\n",
    "    res = {}\n",
    "    words = corpus.words()\n",
    "    \n",
    "    N = len(words)\n",
    "    N_bigrams = N-1\n",
    "    \n",
    "    freq_words = {words[0] : 1}\n",
    "    freq_bigrams = {}\n",
    "\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        curr_word_freq = freq_words.get(words[i], 0)\n",
    "        freq_words[words[i]] = curr_word_freq + 1\n",
    "        \n",
    "        curr_bigram_freq = freq_bigrams.get(words[i-1] + \" \" + words[i], 0)\n",
    "        freq_bigrams[words[i-1] + \" \" + words[i]] = curr_bigram_freq + 1\n",
    "    \n",
    "    \n",
    "    tags = corpus.tagged_words(tagset='universal')\n",
    "    bigram_tags = list(nltk.ngrams(tags, 2))\n",
    "    bigrams_filtered = set(filter(lambda x:x[0][1] in ['NOUN', 'ADJ'] and x[1][1] == 'NOUN', bigram_tags))\n",
    "    \n",
    "    for ((w1, t1), (w2, t2)) in bigrams_filtered:\n",
    "        bigram = w1 + \" \" + w2\n",
    "        w1_prob = freq_words[w1] / N\n",
    "        w2_prob = freq_words[w2] / N\n",
    "        \n",
    "        bigram_prob = freq_bigrams[bigram] / N_bigrams\n",
    "        \n",
    "        t = (bigram_prob - (w1_prob*w2_prob)) / ((bigram_prob / N)**0.5)\n",
    "        if t > threshold:\n",
    "            res[bigram] = t\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def writeToFile(filename, data):\n",
    "    '''\n",
    "    Function that writes data to a csv file.\n",
    "    \n",
    "    Params:\n",
    "    - filename: What the file shall be saved as (remember to include '.csv')\n",
    "    - data: The data to be stored.\n",
    "    '''\n",
    "    file = open(filename, \"w\",  newline='', encoding='utf-8')\n",
    "    data_sorted =  dict(sorted(data.items(), key=lambda item: int(item[1]), reverse=True))\n",
    "    \n",
    "    writer = csv.writer(file)\n",
    "    for key, value in data_sorted.items():\n",
    "        writer.writerow([key, value])\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "def loadData(filename):\n",
    "    '''\n",
    "    Function that reads data from spesified file. This function assumes the file is a correctly formatted csv-file.\n",
    "    \n",
    "    Params:\n",
    "    - filename: The file we want to read.\n",
    "    Output:\n",
    "    - The data in the form of a list. Each entry is a row in the file.\n",
    "    \n",
    "    '''\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "    \n",
    "def getSynonyms(word):\n",
    "    '''\n",
    "    Function to get the synonyms of a given word using WordNet.\n",
    "    \n",
    "    Params:\n",
    "    - word: The word we want the synonyms of.\n",
    "    Output:\n",
    "    - The synonyms of the input word in the form of a list.\n",
    "    '''\n",
    "    synonyms = [] \n",
    "    for syn_set in wordnet.synsets(word): \n",
    "        for l in syn_set.lemmas(): \n",
    "            synonyms.append(l.name()) \n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeToFile(\"freq.csv\", frequencyMethod())\n",
    "writeToFile(\"hyp.csv\", hypothesisMethod(threshold = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def checkBigram(w1, w2, data):\n",
    "    '''\n",
    "    Function that checks if a bigram consisting of two words (w1 and w2) can be replaced with a collocation.\n",
    "    \n",
    "    Params: \n",
    "    - w1, w2: words in bigram we want to investigate.\n",
    "    - data: the data containing all discovered collocations.\n",
    "    Output\n",
    "    ~ Prints collocation if one is found.\n",
    "    - Outputs true if a collocation is found.\n",
    "    '''\n",
    "    for row in data:\n",
    "        w1_true, w2_true = row[0].split()\n",
    "        if w1.lower() in map(lambda x: x.lower(), getSynonyms(w1_true)) != w2.lower() in map(lambda x: x.lower(), getSynonyms(w2_true)):\n",
    "            print(w1_true,w2_true)\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fixCollocation(*args, data_filename = \"hyp.csv\"):\n",
    "    '''\n",
    "    Function that takes a variable size input of words and prints out a preferred collocation for each pair \n",
    "    of adjacent words.\n",
    "    '''\n",
    "    data = loadData(data_filename)\n",
    "    for i in range(1, len(args)):\n",
    "        checkBigram(args[i-1], args[i], data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If a collocation is found it is printed\n",
      "vvvvvvvvv\n",
      "good deal\n"
     ]
    }
   ],
   "source": [
    "print(\"If a collocation is found it is printed\")\n",
    "print('vvvvvvvvv')\n",
    "\n",
    "fixCollocation(\"good\", \"bargain\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If none is found, nothing is printed\n"
     ]
    }
   ],
   "source": [
    "print(\"If none is found, nothing is printed\")\n",
    "fixCollocation(\"How\", \"much\", \"could\", \"a\", \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice:\n",
    "\n",
    "In practice the functions work somewhat alright. The largest flaw is that the corpus is likely not big enough to gather sufficient information about the language to capture the collocations. So sometimes it will come with wrong predictions. I tired to combat this slightly by only allowing a single word to change at a time, as to keep most of the meaning of the bigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
