{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uiwxy3mYV_1b",
    "outputId": "620f7b27-1d46-4613-ed40-68b2cf9fbc81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\paal_\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (0.1.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: requests in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: click in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\paal_\\anaconda3\\lib\\site-packages (0.1.96)\n",
      "Requirement already satisfied: datasets in c:\\users\\paal_\\anaconda3\\lib\\site-packages (1.15.1)\n",
      "Requirement already satisfied: dill in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (20.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (3.8.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (2021.10.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (6.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (1.19.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (0.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from packaging->datasets) (1.15.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\paal_\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LhLeoIiWLMr",
    "outputId": "e8273f97-820f-4c39-afe5-6bfaa3c9a622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sentencepiece as sp\n",
    "from transformers import AlbertTokenizer, AlbertTokenizerFast\n",
    "from transformers import CamembertModel, CamembertConfig, CamembertForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "import math\n",
    "\n",
    "from transformers import (\n",
    "                          CONFIG_MAPPING,\n",
    "                          MODEL_FOR_MASKED_LM_MAPPING,\n",
    "                          MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "                          PreTrainedTokenizer,\n",
    "                          TrainingArguments,\n",
    "                          AutoConfig,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelWithLMHead,\n",
    "                          AutoModelForCausalLM,\n",
    "                          AutoModelForMaskedLM,\n",
    "                          LineByLineTextDataset,\n",
    "                          TextDataset,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorForWholeWordMask,\n",
    "                          DataCollatorForPermutationLanguageModeling,\n",
    "                          PretrainedConfig,\n",
    "                          Trainer,\n",
    "                          set_seed,\n",
    "                          )\n",
    "from datasets import load_dataset\n",
    "set_seed(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sHdH-48WxP5"
   },
   "source": [
    "# Data\n",
    "\n",
    "### Pretraining-data\n",
    "NORWEGIAN: A small section of the OSCAR corpus (1.1GB roughly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vkw7Qbtzavbd"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "#nor_dataset_train_path = '/content/oscar_deduplicatd_no_part_3.txt'\n",
    "nor_dataset_train_path = './content/no_bokmaal-ud-train.txt'\n",
    "nor_dataset_eval_path = './content/no_bokmaal-ud-dev.txt'\n",
    "tokenizer_path = 'norwegian_model/tokenizer'\n",
    "model_path = 'norwegian_model/model'\n",
    "\n",
    "if not os.path.exists('norwegian_model'):\n",
    "    os.makedirs('norwegian_model')\n",
    "    os.makedirs('norwegian_model/tokenizer')\n",
    "    os.makedirs('norwegian_model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ocpn0EDjWsy2"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Kan ikke opprette en fil når filen allerede finnes: 'nor_spiece.model' -> 'norwegian_model/tokenizer/spiece.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1ea5a50eca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'nor_spiece'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSentencePieceTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnor_dataset_train_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.model'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/spiece.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.vocab'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/spiece.vocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#nor_dataset = load_dataset('text', data_files={'train': nor_dataset_train_path})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Kan ikke opprette en fil når filen allerede finnes: 'nor_spiece.model' -> 'norwegian_model/tokenizer/spiece.model'"
     ]
    }
   ],
   "source": [
    "prefix = 'nor_spiece'\n",
    "sp.SentencePieceTrainer.train(input=nor_dataset_train_path, model_prefix=prefix, vocab_size=vocab_size)\n",
    "os.rename(prefix+'.model',tokenizer_path+'/spiece.model')\n",
    "os.rename(prefix+'.vocab',tokenizer_path+'/spiece.vocab')\n",
    "#nor_dataset = load_dataset('text', data_files={'train': nor_dataset_train_path})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30ue4V61fSKd",
    "outputId": "91d5e4a7-d191-4b54-d6c9-cbfccf42da78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('norwegian_model/tokenizer\\\\tokenizer_config.json',\n",
       " 'norwegian_model/tokenizer\\\\special_tokens_map.json',\n",
       " 'norwegian_model/tokenizer\\\\spiece.model',\n",
       " 'norwegian_model/tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norwegian_tokenizer = AlbertTokenizer.from_pretrained(tokenizer_path)\n",
    "norwegian_tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y1BL3JtceW9E"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "configuration = CamembertConfig(\n",
    "    hidden_size = 512,\n",
    "    num_attention_heads = 8,\n",
    "    num_hidden_layers = 10,\n",
    "    max_position_embeddings = 256,\n",
    "    vocab_size = vocab_size,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path+'/output',\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate = 5e-5,\n",
    "    num_train_epochs = 5,\n",
    "    save_steps = 20000/batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKNrKGXpfzTB",
    "outputId": "d88d8e59-b147-435d-e801-a9b0aec8022f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CamembertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 20000\n",
      "}\n",
      "\n",
      "Num parameters 52676640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(20004, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CamembertForMaskedLM(configuration).to(device)\n",
    "print(model.config)\n",
    "print(\"Num parameters\", model.num_parameters())\n",
    "\n",
    "norwegian_tokenizer = AlbertTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Resize model to fit all tokens in tokenizer.\n",
    "model.resize_token_embeddings(len(norwegian_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gVsuNMRgpPP",
    "outputId": "a3c4d607-6054-45ed-9f8f-c3a806d8d5fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=norwegian_tokenizer,\n",
    "    file_path=nor_dataset_train_path,\n",
    "    block_size=256,\n",
    ")\n",
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer=norwegian_tokenizer,\n",
    "    file_path=nor_dataset_eval_path,\n",
    "    block_size=256,\n",
    ")\n",
    "data_collator = DataCollatorForWholeWordMask(\n",
    "    tokenizer=norwegian_tokenizer, \n",
    "    mlm_probability=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6GNr8fN8nok7"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "CTdzw9F1oe7q",
    "outputId": "8503d9d0-1e2b-4d98-f890-894c0b90c531"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 17820\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2785\n",
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:919: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2785' max='2785' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2785/2785 21:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.440000</td>\n",
       "      <td>6.973102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.830800</td>\n",
       "      <td>6.806986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.691100</td>\n",
       "      <td>6.686832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.605700</td>\n",
       "      <td>6.681787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.540300</td>\n",
       "      <td>6.662507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to norwegian_model/model/output\\checkpoint-625\n",
      "Configuration saved in norwegian_model/model/output\\checkpoint-625\\config.json\n",
      "Model weights saved in norwegian_model/model/output\\checkpoint-625\\pytorch_model.bin\n",
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:919: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to norwegian_model/model/output\\checkpoint-1250\n",
      "Configuration saved in norwegian_model/model/output\\checkpoint-1250\\config.json\n",
      "Model weights saved in norwegian_model/model/output\\checkpoint-1250\\pytorch_model.bin\n",
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:919: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to norwegian_model/model/output\\checkpoint-1875\n",
      "Configuration saved in norwegian_model/model/output\\checkpoint-1875\\config.json\n",
      "Model weights saved in norwegian_model/model/output\\checkpoint-1875\\pytorch_model.bin\n",
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:919: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to norwegian_model/model/output\\checkpoint-2500\n",
      "Configuration saved in norwegian_model/model/output\\checkpoint-2500\\config.json\n",
      "Model weights saved in norwegian_model/model/output\\checkpoint-2500\\pytorch_model.bin\n",
      "C:\\Users\\paal_\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:919: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to norwegian_model/model/output\n",
      "Configuration saved in norwegian_model/model/output\\config.json\n",
      "Model weights saved in norwegian_model/model/output\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gwRPFY1coz2S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2621\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate Perplexity:     745.06\n"
     ]
    }
   ],
   "source": [
    "eval_output = trainer.evaluate()\n",
    "# compute perplexity from model loss.\n",
    "\n",
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "INF368_project_new",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
